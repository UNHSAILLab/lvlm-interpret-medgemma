{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T22:08:19.802930Z",
     "start_time": "2025-08-10T22:08:19.635878Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import gc\n",
    "import GPUtil\n",
    "import ast\n",
    "from dataclasses import dataclass"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:02.463593Z",
     "start_time": "2025-08-10T21:48:02.459321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "ce7dbc7dd4808e22",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:03.642518Z",
     "start_time": "2025-08-10T21:48:03.636751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class AttentionConfig:\n",
    "    \"\"\"Configuration for MedGemma attention analysis\"\"\"\n",
    "    image_start: int = 1\n",
    "    image_end: int = 257\n",
    "    num_patches: int = 256\n",
    "    patch_grid_size: int = 16\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 34"
   ],
   "id": "edfba939f03860dc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:06.191148Z",
     "start_time": "2025-08-10T21:48:06.151749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MedGemmaAttentionExtractor:\n",
    "\n",
    "    def __init__(self, model, processor, device='cuda'):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.config = AttentionConfig()\n",
    "        self.model.eval()\n",
    "\n",
    "        # Storage for debugging\n",
    "        self.last_extraction_debug = {}\n",
    "\n",
    "    def prepare_inputs(self, image_path: str, question: str, options: List[str]) -> Dict:\n",
    "        \"\"\"Prepare inputs using your working approach\"\"\"\n",
    "\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Create clinical prompt\n",
    "        prompt = self._create_clinical_prompt(question, options)\n",
    "\n",
    "        # Create messages format that works with MedGemma\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image\", \"image\": image}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Process inputs using your working method\n",
    "        inputs = self.processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move to GPU\n",
    "        inputs_gpu = {\n",
    "            k: v.to(self.device) if torch.is_tensor(v) else v\n",
    "            for k, v in inputs.items()\n",
    "        }\n",
    "\n",
    "        # Store debug info\n",
    "        self.last_extraction_debug['input_length'] = inputs_gpu['input_ids'].shape[1]\n",
    "\n",
    "        return inputs_gpu, image\n",
    "\n",
    "    def _create_clinical_prompt(self, question: str, options: List[str]) -> str:\n",
    "        \"\"\"Create prompt optimized for medical analysis\"\"\"\n",
    "        valid_options = [opt for opt in options if opt]\n",
    "\n",
    "        prompt = f\"\"\"Analyze this chest X-ray and answer: {question}\n",
    "Options: {', '.join(valid_options)}\n",
    "\n",
    "Focus on the relevant anatomical regions for this question.\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def extract_attention_safe(self, outputs, inputs, token_idx: int = 0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Safe extraction based on your working code\n",
    "        Handles MedGemma's fixed attention matrix structure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if token_idx >= len(outputs.attentions):\n",
    "                token_idx = len(outputs.attentions) - 1\n",
    "\n",
    "            # Get attention for this token (last layer)\n",
    "            attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "\n",
    "            if len(attn.shape) == 4:\n",
    "                attn = attn[0]  # Remove batch dimension\n",
    "\n",
    "            # Average over heads\n",
    "            avg_attn = attn.mean(dim=0)\n",
    "\n",
    "            # MedGemma uses fixed size, find the right position\n",
    "            input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "            # The attention matrix doesn't grow, so we use the last valid position\n",
    "            if avg_attn.shape[0] == avg_attn.shape[1]:\n",
    "                gen_pos = min(input_len + token_idx, avg_attn.shape[0] - 1)\n",
    "            else:\n",
    "                gen_pos = avg_attn.shape[0] - 1\n",
    "\n",
    "            # Extract attention to image tokens\n",
    "            if gen_pos >= 0 and self.config.image_end <= avg_attn.shape[1]:\n",
    "                attn_to_image = avg_attn[gen_pos, self.config.image_start:self.config.image_end]\n",
    "\n",
    "                # Verify we got 256 values (16x16 patches)\n",
    "                if len(attn_to_image) == self.config.num_patches:\n",
    "                    return attn_to_image.reshape(self.config.patch_grid_size,\n",
    "                                                self.config.patch_grid_size).numpy()\n",
    "\n",
    "            logger.warning(f\"Could not extract valid attention for token {token_idx}\")\n",
    "            return np.zeros((self.config.patch_grid_size, self.config.patch_grid_size))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting attention: {e}\")\n",
    "            return np.zeros((self.config.patch_grid_size, self.config.patch_grid_size))\n",
    "\n",
    "    def compute_layer_weighted_relevancy(self, outputs, inputs, token_idx: int = 0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute relevancy across all layers with increasing weights for deeper layers\n",
    "        Based on your relevancy analyzer implementation\n",
    "        \"\"\"\n",
    "        relevancy_scores = []\n",
    "\n",
    "        if token_idx < len(outputs.attentions):\n",
    "            token_attentions = outputs.attentions[token_idx]\n",
    "\n",
    "            for layer_idx, layer_attn in enumerate(token_attentions):\n",
    "                if torch.is_tensor(layer_attn):\n",
    "                    layer_attn = layer_attn.cpu().float()\n",
    "\n",
    "                    if len(layer_attn.shape) == 4:\n",
    "                        layer_attn = layer_attn[0]\n",
    "\n",
    "                    # Average over heads\n",
    "                    layer_attn = layer_attn.mean(dim=0)\n",
    "\n",
    "                    # Find position\n",
    "                    input_len = inputs['input_ids'].shape[1]\n",
    "                    src_pos = min(input_len + token_idx, layer_attn.shape[0] - 1)\n",
    "\n",
    "                    # Extract attention to image\n",
    "                    if src_pos >= 0 and self.config.image_end <= layer_attn.shape[1]:\n",
    "                        attn_to_image = layer_attn[src_pos, self.config.image_start:self.config.image_end]\n",
    "\n",
    "                        # Weight by layer depth\n",
    "                        layer_weight = (layer_idx + 1) / len(token_attentions)\n",
    "                        weighted_attn = attn_to_image * layer_weight\n",
    "\n",
    "                        relevancy_scores.append(weighted_attn)\n",
    "\n",
    "        if relevancy_scores:\n",
    "            final_relevancy = torch.stack(relevancy_scores).mean(dim=0)\n",
    "            return final_relevancy.reshape(self.config.patch_grid_size,\n",
    "                                         self.config.patch_grid_size).numpy()\n",
    "\n",
    "        return np.zeros((self.config.patch_grid_size, self.config.patch_grid_size))\n",
    "\n",
    "    def compute_head_importance(self, outputs, inputs, token_idx: int = 0) -> Dict:\n",
    "        \"\"\"Identify which attention heads are most important for this token\"\"\"\n",
    "\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            return {}\n",
    "\n",
    "        # Get last layer attention\n",
    "        last_layer_attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "\n",
    "        if len(last_layer_attn.shape) == 4:\n",
    "            last_layer_attn = last_layer_attn[0]\n",
    "\n",
    "        head_importance = {}\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "        src_pos = min(input_len + token_idx, last_layer_attn.shape[0] - 1)\n",
    "\n",
    "        for h in range(self.config.num_heads):\n",
    "            head_attn = last_layer_attn[h]\n",
    "\n",
    "            if src_pos >= 0 and self.config.image_end <= head_attn.shape[1]:\n",
    "                attn_to_image = head_attn[src_pos, self.config.image_start:self.config.image_end]\n",
    "\n",
    "                # Calculate importance metrics\n",
    "                max_attn = attn_to_image.max().item()\n",
    "                entropy = -(attn_to_image * torch.log(attn_to_image + 1e-10)).sum().item()\n",
    "\n",
    "                head_importance[h] = {\n",
    "                    'max_attention': max_attn,\n",
    "                    'entropy': entropy,\n",
    "                    'focus_score': max_attn * (1 / (1 + entropy))\n",
    "                }\n",
    "\n",
    "        return head_importance"
   ],
   "id": "2bbeec5d2083b8c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:07.842452Z",
     "start_time": "2025-08-10T21:48:07.828436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClinicalAttentionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze attention patterns for clinical interpretation\n",
    "    Maps attention to anatomical regions relevant for specific pathologies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, extractor: MedGemmaAttentionExtractor):\n",
    "        self.extractor = extractor\n",
    "\n",
    "        # Define anatomical regions for 16x16 grid\n",
    "        self.anatomical_regions = {\n",
    "            'upper_left': (0, 5, 0, 5),\n",
    "            'upper_center': (0, 5, 5, 11),\n",
    "            'upper_right': (0, 5, 11, 16),\n",
    "            'middle_left': (5, 11, 0, 5),\n",
    "            'middle_center': (5, 11, 5, 11),\n",
    "            'middle_right': (5, 11, 11, 16),\n",
    "            'lower_left': (11, 16, 0, 5),\n",
    "            'lower_center': (11, 16, 5, 11),\n",
    "            'lower_right': (11, 16, 11, 16)\n",
    "        }\n",
    "\n",
    "        # Map pathologies to expected regions\n",
    "        self.pathology_expectations = {\n",
    "            'pleural effusion': ['lower_left', 'lower_right'],\n",
    "            'pneumothorax': ['upper_left', 'upper_right'],\n",
    "            'consolidation': ['middle_left', 'middle_right', 'lower_left', 'lower_right'],\n",
    "            'cardiomegaly': ['middle_center', 'lower_center'],\n",
    "            'fracture': ['upper_left', 'upper_right', 'middle_left', 'middle_right']\n",
    "        }\n",
    "\n",
    "    def analyze_regional_attention(self, attention_map: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate attention statistics for each anatomical region\"\"\"\n",
    "\n",
    "        regional_stats = {}\n",
    "\n",
    "        for region_name, (r1, r2, c1, c2) in self.anatomical_regions.items():\n",
    "            region_attention = attention_map[r1:r2, c1:c2]\n",
    "\n",
    "            regional_stats[region_name] = {\n",
    "                'mean': float(region_attention.mean()),\n",
    "                'max': float(region_attention.max()),\n",
    "                'sum': float(region_attention.sum()),\n",
    "                'std': float(region_attention.std())\n",
    "            }\n",
    "\n",
    "        # Sort by mean attention\n",
    "        sorted_regions = sorted(regional_stats.items(),\n",
    "                              key=lambda x: x[1]['mean'],\n",
    "                              reverse=True)\n",
    "\n",
    "        regional_stats['top_regions'] = [r[0] for r in sorted_regions[:3]]\n",
    "\n",
    "        return regional_stats\n",
    "\n",
    "    def evaluate_pathology_alignment(self, attention_map: np.ndarray,\n",
    "                                    pathology: str) -> Dict:\n",
    "        \"\"\"Evaluate if attention aligns with expected regions for pathology\"\"\"\n",
    "\n",
    "        regional_stats = self.analyze_regional_attention(attention_map)\n",
    "\n",
    "        if pathology.lower() in self.pathology_expectations:\n",
    "            expected_regions = self.pathology_expectations[pathology.lower()]\n",
    "\n",
    "            # Calculate attention in expected vs unexpected regions\n",
    "            expected_attention = sum(\n",
    "                regional_stats[r]['mean']\n",
    "                for r in expected_regions\n",
    "                if r in regional_stats\n",
    "            )\n",
    "\n",
    "            all_attention = sum(\n",
    "                stats['mean']\n",
    "                for r, stats in regional_stats.items()\n",
    "                if r != 'top_regions'\n",
    "            )\n",
    "\n",
    "            alignment_score = expected_attention / (all_attention + 1e-8)\n",
    "\n",
    "            # Check if top regions overlap with expected\n",
    "            top_regions = regional_stats.get('top_regions', [])\n",
    "            overlap = len(set(top_regions) & set(expected_regions))\n",
    "\n",
    "            return {\n",
    "                'alignment_score': float(alignment_score),\n",
    "                'expected_regions': expected_regions,\n",
    "                'actual_top_regions': top_regions,\n",
    "                'overlap_count': overlap,\n",
    "                'is_aligned': overlap >= 1\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'alignment_score': 0.0,\n",
    "            'expected_regions': [],\n",
    "            'actual_top_regions': regional_stats.get('top_regions', []),\n",
    "            'overlap_count': 0,\n",
    "            'is_aligned': False\n",
    "        }"
   ],
   "id": "880f5b6a0bb67a60",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:11.838360Z",
     "start_time": "2025-08-10T21:48:11.825684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ViTPrismaIntegration:\n",
    "    \"\"\"\n",
    "    Integrate ViT-Prisma concepts with MedGemma\n",
    "    Attention rollout and diversity analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, extractor: MedGemmaAttentionExtractor):\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def compute_attention_rollout(self, outputs, inputs, token_idx: int = 0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute attention rollout across layers\n",
    "        Shows how information flows through the network\n",
    "        \"\"\"\n",
    "\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "        token_attentions = outputs.attentions[token_idx]\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        # Initialize with identity\n",
    "        rollout = None\n",
    "\n",
    "        for layer_idx, layer_attn in enumerate(token_attentions):\n",
    "            if torch.is_tensor(layer_attn):\n",
    "                layer_attn = layer_attn.cpu().float()\n",
    "\n",
    "                if len(layer_attn.shape) == 4:\n",
    "                    layer_attn = layer_attn[0]\n",
    "\n",
    "                # Average over heads\n",
    "                layer_attn = layer_attn.mean(dim=0)\n",
    "\n",
    "                # Add residual connection\n",
    "                eye = torch.eye(layer_attn.shape[0])\n",
    "                layer_attn = 0.5 * layer_attn + 0.5 * eye\n",
    "\n",
    "                # Normalize\n",
    "                layer_attn = layer_attn / layer_attn.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                if rollout is None:\n",
    "                    rollout = layer_attn\n",
    "                else:\n",
    "                    rollout = torch.matmul(rollout, layer_attn)\n",
    "\n",
    "        if rollout is not None:\n",
    "            # Extract final attention to image\n",
    "            src_pos = min(input_len + token_idx, rollout.shape[0] - 1)\n",
    "\n",
    "            if src_pos >= 0 and self.extractor.config.image_end <= rollout.shape[1]:\n",
    "                attn_to_image = rollout[src_pos,\n",
    "                                       self.extractor.config.image_start:self.extractor.config.image_end]\n",
    "                return attn_to_image.reshape(16, 16).numpy()\n",
    "\n",
    "        return np.zeros((16, 16))\n",
    "\n",
    "    def compute_attention_diversity(self, outputs, inputs, num_tokens: int = 5) -> float:\n",
    "        \"\"\"\n",
    "        Measure diversity of attention patterns across tokens\n",
    "        Higher diversity suggests model examines different aspects\n",
    "        \"\"\"\n",
    "\n",
    "        attention_maps = []\n",
    "\n",
    "        for token_idx in range(min(num_tokens, len(outputs.attentions))):\n",
    "            attn_map = self.extractor.extract_attention_safe(outputs, inputs, token_idx)\n",
    "            if attn_map.max() > 0:  # Valid attention map\n",
    "                attention_maps.append(attn_map.flatten())\n",
    "\n",
    "        if len(attention_maps) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        distances = []\n",
    "        for i in range(len(attention_maps)):\n",
    "            for j in range(i + 1, len(attention_maps)):\n",
    "                dist = np.linalg.norm(attention_maps[i] - attention_maps[j])\n",
    "                distances.append(dist)\n",
    "\n",
    "        return float(np.mean(distances))"
   ],
   "id": "c9d640ba63040756",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:15.380707Z",
     "start_time": "2025-08-10T21:48:15.358981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MIMICCXRProcessor:\n",
    "    \"\"\"\n",
    "    Process MIMIC-CXR data with comprehensive attention analysis\n",
    "    Builds on your existing code structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, processor, base_image_path: str, device='cuda'):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.base_image_path = Path(base_image_path)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize components\n",
    "        self.extractor = MedGemmaAttentionExtractor(model, processor, device)\n",
    "        self.clinical_analyzer = ClinicalAttentionAnalyzer(self.extractor)\n",
    "        self.prisma = ViTPrismaIntegration(self.extractor)\n",
    "\n",
    "        # Storage for results\n",
    "        self.results_cache = {}\n",
    "\n",
    "    def process_single_sample(self, row: pd.Series) -> Dict:\n",
    "        \"\"\"Process a single MIMIC-CXR sample\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            image_path = self.base_image_path / row['ImagePath']\n",
    "            if not image_path.exists():\n",
    "                logger.warning(f\"Image not found: {image_path}\")\n",
    "                return None\n",
    "\n",
    "            inputs_gpu, image = self.extractor.prepare_inputs(\n",
    "                str(image_path),\n",
    "                row['question'],\n",
    "                ast.literal_eval(row['options']) if isinstance(row['options'], str) else row['options']\n",
    "            )\n",
    "\n",
    "            # Generate with attention using your working config\n",
    "            gen_kwargs = {\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"do_sample\": False,  # Greedy as per your findings\n",
    "                \"output_attentions\": True,\n",
    "                \"return_dict_in_generate\": True,\n",
    "                \"pad_token_id\": self.processor.tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": self.processor.tokenizer.eos_token_id,\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs_gpu, **gen_kwargs)\n",
    "\n",
    "            # Extract generated text\n",
    "            generated_ids = outputs.sequences[0][len(inputs_gpu['input_ids'][0]):]\n",
    "            generated_text = self.processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Attention analysis\n",
    "            attention_results = self.analyze_attention(outputs, inputs_gpu, row)\n",
    "\n",
    "            # Compile results\n",
    "            result = {\n",
    "                'study_id': row['study_id'],\n",
    "                'question': row['question'],\n",
    "                'correct_answer': row['correct_answer'],\n",
    "                'generated_answer': self._extract_answer(generated_text, row['options']),\n",
    "                'generated_text': generated_text,\n",
    "                'attention_analysis': attention_results,\n",
    "                'image_path': str(image_path)\n",
    "            }\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing sample {row['study_id']}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_attention(self, outputs, inputs, row) -> Dict:\n",
    "        \"\"\"Comprehensive attention analysis for a sample\"\"\"\n",
    "\n",
    "        # Extract pathology from question\n",
    "        pathology = self._extract_pathology(row['question'])\n",
    "\n",
    "        # 1. Raw attention extraction\n",
    "        raw_attention = self.extractor.extract_attention_safe(outputs, inputs, 0)\n",
    "\n",
    "        # 2. Layer-weighted relevancy\n",
    "        relevancy = self.extractor.compute_layer_weighted_relevancy(outputs, inputs, 0)\n",
    "\n",
    "        # 3. Head importance\n",
    "        head_importance = self.extractor.compute_head_importance(outputs, inputs, 0)\n",
    "\n",
    "        # 4. Attention rollout\n",
    "        rollout = self.prisma.compute_attention_rollout(outputs, inputs, 0)\n",
    "\n",
    "        # 5. Regional analysis\n",
    "        regional_stats = self.clinical_analyzer.analyze_regional_attention(relevancy)\n",
    "\n",
    "        # 6. Pathology alignment\n",
    "        alignment = self.clinical_analyzer.evaluate_pathology_alignment(relevancy, pathology)\n",
    "\n",
    "        # 7. Attention diversity\n",
    "        diversity = self.prisma.compute_attention_diversity(outputs, inputs)\n",
    "\n",
    "        return {\n",
    "            'raw_attention': raw_attention.tolist(),\n",
    "            'relevancy_map': relevancy.tolist(),\n",
    "            'attention_rollout': rollout.tolist(),\n",
    "            'head_importance': head_importance,\n",
    "            'regional_stats': regional_stats,\n",
    "            'pathology_alignment': alignment,\n",
    "            'attention_diversity': diversity,\n",
    "            'pathology_type': pathology\n",
    "        }\n",
    "\n",
    "    def _extract_pathology(self, question: str) -> str:\n",
    "        \"\"\"Extract pathology type from question\"\"\"\n",
    "        question_lower = question.lower()\n",
    "\n",
    "        pathologies = ['pleural effusion', 'pneumothorax', 'consolidation',\n",
    "                      'opacity', 'fracture', 'hernia', 'kyphosis', 'cardiomegaly']\n",
    "\n",
    "        for pathology in pathologies:\n",
    "            if pathology in question_lower:\n",
    "                return pathology\n",
    "\n",
    "        if 'effusion' in question_lower:\n",
    "            return 'pleural effusion'\n",
    "\n",
    "        return 'other'\n",
    "\n",
    "    def _extract_answer(self, generated_text: str, options: List[str]) -> str:\n",
    "        \"\"\"Extract answer from generated text\"\"\"\n",
    "        text_lower = generated_text.lower()\n",
    "\n",
    "        # Simple heuristic - look for yes/no\n",
    "        if 'yes' in text_lower[:20]:\n",
    "            return 'yes'\n",
    "        elif 'no' in text_lower[:20]:\n",
    "            return 'no'\n",
    "\n",
    "        return 'uncertain'\n",
    "\n",
    "    def process_dataset(self, df: pd.DataFrame, sample_size: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Process entire dataset\"\"\"\n",
    "\n",
    "        if sample_size:\n",
    "            df = df.sample(min(sample_size, len(df)))\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing samples\"):\n",
    "            result = self.process_single_sample(row)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "\n",
    "            # Periodic cleanup\n",
    "            if idx % 10 == 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return pd.DataFrame(results)"
   ],
   "id": "a5f13dbe780b4d6f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:19.320298Z",
     "start_time": "2025-08-10T21:48:19.293054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_clinical_visualization(image_path: str, attention_map: np.ndarray,\n",
    "                                regional_stats: Dict, alignment: Dict,\n",
    "                                question: str, generated_answer: str) -> plt.Figure:\n",
    "    \"\"\"Create comprehensive clinical visualization\"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # 1. Original X-ray\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(image, cmap='gray')\n",
    "    ax1.set_title('Input X-ray')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # 2. Attention heatmap\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    im = ax2.imshow(attention_map, cmap='hot', interpolation='bicubic')\n",
    "    ax2.set_title('Attention Map')\n",
    "    ax2.axis('off')\n",
    "    plt.colorbar(im, ax=ax2, fraction=0.046)\n",
    "\n",
    "    # 3. Overlay\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.imshow(image, cmap='gray')\n",
    "\n",
    "    # Resize attention to image size\n",
    "    h, w = image.size[::-1]\n",
    "    attention_resized = cv2.resize(attention_map, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "    ax3.imshow(attention_resized, cmap='hot', alpha=0.5)\n",
    "    ax3.set_title('Attention Overlay')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    # 4. Regional analysis bar chart\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    regions = list(regional_stats.keys())\n",
    "    if 'top_regions' in regions:\n",
    "        regions.remove('top_regions')\n",
    "    values = [regional_stats[r]['mean'] for r in regions]\n",
    "\n",
    "    bars = ax4.bar(range(len(regions)), values, color='skyblue')\n",
    "    ax4.set_xticks(range(len(regions)))\n",
    "    ax4.set_xticklabels([r.replace('_', '\\n') for r in regions], rotation=0, fontsize=8)\n",
    "    ax4.set_ylabel('Mean Attention')\n",
    "    ax4.set_title('Regional Attention Distribution')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # Highlight top regions\n",
    "    top_regions = regional_stats.get('top_regions', [])\n",
    "    for i, region in enumerate(regions):\n",
    "        if region in top_regions:\n",
    "            bars[i].set_color('coral')\n",
    "\n",
    "    # 5. Pathology alignment\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    ax5.axis('off')\n",
    "\n",
    "    alignment_text = f\"Question: {question[:50]}...\\n\\n\"\n",
    "    alignment_text += f\"Generated Answer: {generated_answer}\\n\\n\"\n",
    "    alignment_text += \"Pathology Alignment:\\n\"\n",
    "    alignment_text += f\"• Alignment Score: {alignment.get('alignment_score', 0):.2%}\\n\"\n",
    "    alignment_text += f\"• Expected Regions: {', '.join(alignment.get('expected_regions', []))}\\n\"\n",
    "    alignment_text += f\"• Actual Top Regions: {', '.join(alignment.get('actual_top_regions', []))}\\n\"\n",
    "    alignment_text += f\"• Clinically Aligned: {'✓' if alignment.get('is_aligned') else '✗'}\"\n",
    "\n",
    "    ax5.text(0.05, 0.95, alignment_text, transform=ax5.transAxes,\n",
    "            fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "    # 6. Attention statistics\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax6.hist(attention_map.flatten(), bins=30, alpha=0.7, color='purple')\n",
    "    ax6.axvline(attention_map.mean(), color='red', linestyle='--',\n",
    "               label=f'Mean: {attention_map.mean():.3f}')\n",
    "    ax6.axvline(np.percentile(attention_map, 90), color='green', linestyle='--',\n",
    "               label=f'90th %ile: {np.percentile(attention_map, 90):.3f}')\n",
    "    ax6.set_xlabel('Attention Value')\n",
    "    ax6.set_ylabel('Frequency')\n",
    "    ax6.set_title('Attention Distribution')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('MedGemma Clinical Attention Analysis', fontsize=14)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Main execution function\n",
    "def run_mimic_analysis(csv_path: str, image_base_path: str,\n",
    "                       output_dir: str, sample_size: Optional[int] = None):\n",
    "    \"\"\"Run complete MIMIC-CXR analysis with MedGemma\"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"MEDGEMMA MIMIC-CXR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Set up output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Initialize model (using your working configuration)\n",
    "    print(\"\\n1. Loading MedGemma model...\")\n",
    "    model_id = 'google/medgemma-4b-it'\n",
    "\n",
    "    from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\",\n",
    "        attn_implementation=\"eager\",\n",
    "        tie_word_embeddings=False\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\n2. Loading MIMIC-CXR data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"✓ Loaded {len(df)} samples\")\n",
    "\n",
    "    # Initialize processor\n",
    "    print(\"\\n3. Initializing processor...\")\n",
    "    mimic_processor = MIMICCXRProcessor(model, processor, image_base_path)\n",
    "\n",
    "    # Process dataset\n",
    "    print(f\"\\n4. Processing {sample_size if sample_size else len(df)} samples...\")\n",
    "    results_df = mimic_processor.process_dataset(df, sample_size)\n",
    "\n",
    "    # Save results\n",
    "    print(\"\\n5. Saving results...\")\n",
    "    results_df.to_csv(output_path / 'attention_results.csv', index=False)\n",
    "\n",
    "    # Analyze results\n",
    "    print(\"\\n6. Analyzing results...\")\n",
    "\n",
    "    # Pathology alignment statistics\n",
    "    if 'attention_analysis' in results_df.columns:\n",
    "        alignments = []\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row['attention_analysis'] and 'pathology_alignment' in row['attention_analysis']:\n",
    "                alignments.append(row['attention_analysis']['pathology_alignment']['alignment_score'])\n",
    "\n",
    "        if alignments:\n",
    "            print(f\"\\nPathology Alignment Statistics:\")\n",
    "            print(f\"  Mean alignment: {np.mean(alignments):.2%}\")\n",
    "            print(f\"  Std alignment: {np.std(alignments):.2%}\")\n",
    "            print(f\"  Max alignment: {np.max(alignments):.2%}\")\n",
    "\n",
    "    # Generate visualizations for top samples\n",
    "    print(\"\\n7. Creating visualizations...\")\n",
    "    viz_dir = output_path / 'visualizations'\n",
    "    viz_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for idx, row in results_df.head(5).iterrows():\n",
    "        if row['attention_analysis']:\n",
    "            attention_map = np.array(row['attention_analysis']['relevancy_map'])\n",
    "\n",
    "            fig = create_clinical_visualization(\n",
    "                row['image_path'],\n",
    "                attention_map,\n",
    "                row['attention_analysis']['regional_stats'],\n",
    "                row['attention_analysis']['pathology_alignment'],\n",
    "                row['question'],\n",
    "                row['generated_answer']\n",
    "            )\n",
    "\n",
    "            fig.savefig(viz_dir / f'sample_{idx}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"\\n✓ Analysis complete! Results saved to {output_path}\")\n",
    "\n",
    "    return results_df"
   ],
   "id": "3f411e3879409323",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:48:52.065957Z",
     "start_time": "2025-08-10T21:48:45.365248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_path = \"/home/bsada1/lvlm-interpret-medgemma/one-pixel-attack/mimic_adapted_questions.csv\"\n",
    "image_base_path = \"/home/bsada1/mimic_cxr_hundred_vqa\"\n",
    "output_dir = \"mimic_medgemma_analysis\"\n",
    "results = run_mimic_analysis(csv_path, image_base_path, output_dir, sample_size=10)"
   ],
   "id": "adb89e2b7d466205",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MEDGEMMA MIMIC-CXR ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. Loading MedGemma model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "\n",
      "2. Loading MIMIC-CXR data...\n",
      "✓ Loaded 100 samples\n",
      "\n",
      "3. Initializing processor...\n",
      "\n",
      "4. Processing 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/10 [00:00<?, ?it/s]ERROR:__main__:Error processing sample a6617202-f5a8661d-78eb1442-037bf3e4-3dd8967f: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 626.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.09 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 24.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  10%|█         | 1/10 [00:00<00:03,  2.29it/s]ERROR:__main__:Error processing sample 051b7911-cb00aec9-0b309188-89803662-303ec278: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  20%|██        | 2/10 [00:00<00:02,  3.72it/s]ERROR:__main__:Error processing sample edb88e4a-c04f1be7-aefcf3e0-8889542d-692ff7fd: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  30%|███       | 3/10 [00:00<00:01,  4.74it/s]ERROR:__main__:Error processing sample 9ca8f84e-92fac212-e60ac49d-01779362-caa16791: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  40%|████      | 4/10 [00:00<00:01,  5.34it/s]ERROR:__main__:Error processing sample d999236f-95dcb8b7-a4d20a3f-be538f50-ce13a08e: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  50%|█████     | 5/10 [00:01<00:00,  5.67it/s]ERROR:__main__:Error processing sample 2833b85f-3bb4273f-cffd3794-2bf2cd57-7ddb3f5f: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  60%|██████    | 6/10 [00:01<00:00,  5.79it/s]ERROR:__main__:Error processing sample 40a4d537-de28a3ab-aa746c28-750aea7d-2965bcdb: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  70%|███████   | 7/10 [00:01<00:00,  4.48it/s]ERROR:__main__:Error processing sample d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 620.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.10 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  80%|████████  | 8/10 [00:01<00:00,  4.97it/s]ERROR:__main__:Error processing sample c7526473-7b7214ee-a5d58d12-29d1f67f-9f4edf00: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 108.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.60 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 542.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples:  90%|█████████ | 9/10 [00:01<00:00,  5.69it/s]ERROR:__main__:Error processing sample 38f6981f-10343eb8-7c974e21-458d5218-f29f1617: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 108.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.60 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 542.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Processing samples: 100%|██████████| 10/10 [00:01<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Saving results...\n",
      "\n",
      "6. Analyzing results...\n",
      "\n",
      "7. Creating visualizations...\n",
      "\n",
      "✓ Analysis complete! Results saved to mimic_medgemma_analysis\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "i Loading MedGemma model...\n",
    "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]\n",
    "\n",
    "✓ Model loaded successfully\n",
    "\n",
    "2. Loading MIMIC-CXR data...\n",
    "✓ Loaded 100 samples\n",
    "\n",
    "3. Initializing processor...\n",
    "\n",
    "4. Processing 10 samples...\n",
    "Processing samples:   0%|          | 0/10 [00:00<?, ?it/s]ERROR:__main__:Error processing sample a6617202-f5a8661d-78eb1442-037bf3e4-3dd8967f: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 626.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.09 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 24.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "Processing samples:  10%|█         | 1/10 [00:00<00:03,  2.29it/s]ERROR:__main__:Error processing sample 051b7911-cb00aec9-0b309188-89803662-303ec278: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "Processing samples:  20%|██        | 2/10 [00:00<00:02,  3.72it/s]ERROR:__main__:Error processing sample edb88e4a-c04f1be7-aefcf3e0-8889542d-692ff7fd: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "Processing samples:  30%|███       | 3/10 [00:00<00:01,  4.74it/s]ERROR:__main__:Error processing sample 9ca8f84e-92fac212-e60ac49d-01779362-caa16791: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "Processing samples:  40%|████      | 4/10 [00:00<00:01,  5.34it/s]ERROR:__main__:Error processing sample d999236f-95dcb8b7-a4d20a3f-be538f50-ce13a08e: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "Processing samples:  50%|█████     | 5/10 [00:01<00:00,  5.67it/s]ERROR:__main__:Error processing sample 2833b85f-3bb4273f-cffd3794-2bf2cd57-7ddb3f5f: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 114.50 MiB is free. Process 3818168 has 672.00 MiB memory in use. Process 2843597 has 66.58 GiB memory in use. Process 2985595 has 1.28 GiB memory in use. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 9.57 GiB is allocated by PyTorch, and 536.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLO"
   ],
   "id": "4508edd22988a03c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:58:08.273675Z",
     "start_time": "2025-08-10T21:58:08.218299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up environment for memory efficiency\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def select_best_gpu(min_free_memory_gb: float = 20.0) -> int:\n",
    "    \"\"\"\n",
    "    Select GPU with most free memory\n",
    "    Returns GPU index or raises error if none available\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        best_gpu = None\n",
    "        max_free_memory = 0\n",
    "\n",
    "        print(\"\\n=== GPU Memory Status ===\")\n",
    "        for gpu in gpus:\n",
    "            free_memory_gb = gpu.memoryFree / 1024\n",
    "            used_memory_gb = gpu.memoryUsed / 1024\n",
    "            total_memory_gb = gpu.memoryTotal / 1024\n",
    "\n",
    "            print(f\"GPU {gpu.id}: {free_memory_gb:.1f}GB free / {total_memory_gb:.1f}GB total \"\n",
    "                  f\"({gpu.memoryUtil*100:.1f}% used)\")\n",
    "\n",
    "            if free_memory_gb > max_free_memory and free_memory_gb >= min_free_memory_gb:\n",
    "                max_free_memory = free_memory_gb\n",
    "                best_gpu = gpu.id\n",
    "\n",
    "        if best_gpu is None:\n",
    "            raise RuntimeError(f\"No GPU with at least {min_free_memory_gb}GB free memory available\")\n",
    "\n",
    "        print(f\"\\n✓ Selected GPU {best_gpu} with {max_free_memory:.1f}GB free memory\")\n",
    "        return best_gpu\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error selecting GPU: {e}\")\n",
    "        print(\"Falling back to GPU 1 (seems to have more free memory)\")\n",
    "        return 1\n",
    "\n",
    "@dataclass\n",
    "class MemoryConfig:\n",
    "    \"\"\"Configuration for memory-efficient processing\"\"\"\n",
    "    max_new_tokens: int = 30  # Reduced from 50\n",
    "    attention_layers_to_save: List[int] = None  # Only save specific layers\n",
    "    max_attention_tokens: int = 10  # Only save attention for first N tokens\n",
    "    batch_size: int = 1\n",
    "    clear_cache_frequency: int = 5  # Clear cache every N samples\n",
    "    use_amp: bool = True  # Use automatic mixed precision\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.attention_layers_to_save is None:\n",
    "            # Only save first, middle, and last layer by default\n",
    "            self.attention_layers_to_save = [0, 17, 33]\n",
    "\n",
    "class MemoryEfficientMedGemmaProcessor:\n",
    "    \"\"\"\n",
    "    Memory-optimized version of MedGemma processor\n",
    "    Key optimizations:\n",
    "    1. Selective attention saving (not all layers/tokens)\n",
    "    2. Automatic GPU selection\n",
    "    3. Mixed precision inference\n",
    "    4. Aggressive cache clearing\n",
    "    5. Gradient checkpointing disabled (inference only)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str = 'google/medgemma-4b-it',\n",
    "                 device_id: Optional[int] = None,\n",
    "                 memory_config: Optional[MemoryConfig] = None):\n",
    "\n",
    "        self.memory_config = memory_config or MemoryConfig()\n",
    "\n",
    "        # Select best GPU if not specified\n",
    "        if device_id is None:\n",
    "            device_id = select_best_gpu()\n",
    "\n",
    "        self.device = f'cuda:{device_id}'\n",
    "        print(f\"\\nInitializing MedGemma on {self.device}\")\n",
    "\n",
    "        # Load model with memory optimizations\n",
    "        self._load_model(model_id)\n",
    "\n",
    "        # Initialize components\n",
    "        self.attention_cache = {}\n",
    "        self.samples_processed = 0\n",
    "\n",
    "    def _load_model(self, model_id: str):\n",
    "        \"\"\"Load model with memory-efficient settings\"\"\"\n",
    "        from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "        print(\"Loading processor...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        print(\"Loading model with memory optimizations...\")\n",
    "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency\n",
    "            device_map=self.device,\n",
    "            attn_implementation=\"eager\",\n",
    "            tie_word_embeddings=False,\n",
    "            low_cpu_mem_usage=True,  # Reduce CPU memory usage during loading\n",
    "        )\n",
    "\n",
    "        # Set to eval mode and disable gradient computation\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(\"✓ Model loaded successfully\")\n",
    "\n",
    "        # Print memory usage after loading\n",
    "        self._print_memory_usage()\n",
    "\n",
    "    def _print_memory_usage(self):\n",
    "        \"\"\"Print current GPU memory usage\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(self.device) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(self.device) / 1024**3\n",
    "            print(f\"GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved\")\n",
    "\n",
    "    def process_single_sample_efficient(self, image_path: str, question: str,\n",
    "                                       options: List[str], study_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single sample with aggressive memory management\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "            # Create prompt\n",
    "            valid_options = [opt for opt in options if opt]\n",
    "            prompt = f\"\"\"Analyze this chest X-ray and answer: {question}\n",
    "Options: {', '.join(valid_options)}\n",
    "Answer with just 'yes' or 'no'.\"\"\"\n",
    "\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\"type\": \"image\", \"image\": image}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Process inputs\n",
    "            inputs = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Move to GPU\n",
    "            inputs_gpu = {\n",
    "                k: v.to(self.device) if torch.is_tensor(v) else v\n",
    "                for k, v in inputs.items()\n",
    "            }\n",
    "\n",
    "            # Generate with limited tokens and selective attention saving\n",
    "            with torch.cuda.amp.autocast(enabled=self.memory_config.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs_gpu,\n",
    "                        max_new_tokens=self.memory_config.max_new_tokens,\n",
    "                        do_sample=False,\n",
    "                        output_attentions=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "            # Extract generated text\n",
    "            generated_ids = outputs.sequences[0][len(inputs['input_ids'][0]):]\n",
    "            generated_text = self.processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Extract answer\n",
    "            answer = self._extract_answer(generated_text)\n",
    "\n",
    "            # Selective attention extraction (memory efficient)\n",
    "            attention_summary = self._extract_attention_efficient(outputs, inputs_gpu)\n",
    "\n",
    "            # Clean up intermediate tensors\n",
    "            del outputs\n",
    "            del inputs_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return {\n",
    "                'study_id': study_id,\n",
    "                'question': question,\n",
    "                'generated_answer': answer,\n",
    "                'generated_text': generated_text[:100],  # Truncate for storage\n",
    "                'attention_summary': attention_summary\n",
    "            }\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            logger.error(f\"OOM for sample {study_id}: {e}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing sample {study_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_attention_efficient(self, outputs, inputs) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract only essential attention information to save memory\n",
    "        Instead of saving all attention tensors, we compute statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not hasattr(outputs, 'attentions') or not outputs.attentions:\n",
    "                return {}\n",
    "\n",
    "            # Only process first few tokens\n",
    "            num_tokens = min(len(outputs.attentions), self.memory_config.max_attention_tokens)\n",
    "\n",
    "            attention_stats = {\n",
    "                'num_tokens_analyzed': num_tokens,\n",
    "                'regional_focus': [],\n",
    "                'attention_entropy': []\n",
    "            }\n",
    "\n",
    "            for token_idx in range(num_tokens):\n",
    "                if token_idx >= len(outputs.attentions):\n",
    "                    break\n",
    "\n",
    "                # Get last layer attention only (most semantic)\n",
    "                last_layer_attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "\n",
    "                if len(last_layer_attn.shape) == 4:\n",
    "                    last_layer_attn = last_layer_attn[0]\n",
    "\n",
    "                # Average over heads\n",
    "                avg_attn = last_layer_attn.mean(dim=0)\n",
    "\n",
    "                # Extract attention to image region\n",
    "                input_len = inputs['input_ids'].shape[1]\n",
    "                src_pos = min(input_len + token_idx, avg_attn.shape[0] - 1)\n",
    "\n",
    "                if src_pos >= 0 and 257 <= avg_attn.shape[1]:\n",
    "                    attn_to_image = avg_attn[src_pos, 1:257]\n",
    "\n",
    "                    # Compute statistics instead of saving tensor\n",
    "                    attn_2d = attn_to_image.reshape(16, 16).numpy()\n",
    "\n",
    "                    # Regional analysis (which quadrant has most attention)\n",
    "                    quadrants = {\n",
    "                        'upper_left': attn_2d[:8, :8].mean(),\n",
    "                        'upper_right': attn_2d[:8, 8:].mean(),\n",
    "                        'lower_left': attn_2d[8:, :8].mean(),\n",
    "                        'lower_right': attn_2d[8:, 8:].mean()\n",
    "                    }\n",
    "\n",
    "                    max_quadrant = max(quadrants, key=quadrants.get)\n",
    "                    attention_stats['regional_focus'].append(max_quadrant)\n",
    "\n",
    "                    # Attention entropy (how distributed vs focused)\n",
    "                    entropy = stats.entropy(attn_2d.flatten() + 1e-10)\n",
    "                    attention_stats['attention_entropy'].append(float(entropy))\n",
    "\n",
    "            return attention_stats\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting attention: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _extract_answer(self, text: str) -> str:\n",
    "        \"\"\"Extract yes/no answer from generated text\"\"\"\n",
    "        text_lower = text.lower()[:50]  # Only check beginning\n",
    "\n",
    "        if 'yes' in text_lower:\n",
    "            return 'yes'\n",
    "        elif 'no' in text_lower:\n",
    "            return 'no'\n",
    "        else:\n",
    "            return 'uncertain'\n",
    "\n",
    "    def process_dataset_batch(self, df: pd.DataFrame, image_base_path: str,\n",
    "                             output_dir: str, sample_size: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Process dataset with batching and memory management\n",
    "        \"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Sample if requested\n",
    "        if sample_size:\n",
    "            df = df.sample(min(sample_size, len(df)), random_state=42)\n",
    "\n",
    "        results = []\n",
    "        failed_samples = []\n",
    "\n",
    "        print(f\"\\nProcessing {len(df)} samples...\")\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "            # Prepare data\n",
    "            image_path = Path(image_base_path) / row['ImagePath']\n",
    "\n",
    "            if not image_path.exists():\n",
    "                logger.warning(f\"Image not found: {image_path}\")\n",
    "                failed_samples.append(row['study_id'])\n",
    "                continue\n",
    "\n",
    "            # Parse options\n",
    "            if isinstance(row['options'], str):\n",
    "                options = ast.literal_eval(row['options'])\n",
    "            else:\n",
    "                options = row['options']\n",
    "\n",
    "            # Process sample\n",
    "            result = self.process_single_sample_efficient(\n",
    "                str(image_path),\n",
    "                row['question'],\n",
    "                options,\n",
    "                row['study_id']\n",
    "            )\n",
    "\n",
    "            if result:\n",
    "                result['correct_answer'] = row['correct_answer']\n",
    "                result['is_correct'] = result['generated_answer'] == row['correct_answer']\n",
    "                results.append(result)\n",
    "            else:\n",
    "                failed_samples.append(row['study_id'])\n",
    "\n",
    "            # Periodic memory cleanup\n",
    "            self.samples_processed += 1\n",
    "            if self.samples_processed % self.memory_config.clear_cache_frequency == 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"\\n[Memory cleanup at sample {self.samples_processed}]\")\n",
    "                self._print_memory_usage()\n",
    "\n",
    "        # Save results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_path / 'results.csv', index=False)\n",
    "\n",
    "        # Save failed samples\n",
    "        if failed_samples:\n",
    "            with open(output_path / 'failed_samples.txt', 'w') as f:\n",
    "                f.write('\\n'.join(failed_samples))\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Successful: {len(results)}/{len(df)}\")\n",
    "        print(f\"Failed: {len(failed_samples)}\")\n",
    "\n",
    "        if len(results) > 0:\n",
    "            accuracy = results_df['is_correct'].mean()\n",
    "            print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "            # Analyze attention patterns\n",
    "            self._analyze_attention_patterns(results_df)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    def _analyze_attention_patterns(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Analyze attention patterns from results\"\"\"\n",
    "        print(\"\\n=== Attention Pattern Analysis ===\")\n",
    "\n",
    "        # Extract regional focus patterns\n",
    "        all_regions = []\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row['attention_summary'] and 'regional_focus' in row['attention_summary']:\n",
    "                all_regions.extend(row['attention_summary']['regional_focus'])\n",
    "\n",
    "        if all_regions:\n",
    "            from collections import Counter\n",
    "            region_counts = Counter(all_regions)\n",
    "            total = len(all_regions)\n",
    "\n",
    "            print(\"\\nRegional Focus Distribution:\")\n",
    "            for region, count in region_counts.most_common():\n",
    "                percentage = (count / total) * 100\n",
    "                print(f\"  {region}: {percentage:.1f}%\")\n",
    "\n",
    "        # Analyze entropy\n",
    "        all_entropies = []\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row['attention_summary'] and 'attention_entropy' in row['attention_summary']:\n",
    "                all_entropies.extend(row['attention_summary']['attention_entropy'])\n",
    "\n",
    "        if all_entropies:\n",
    "            print(f\"\\nAttention Entropy Statistics:\")\n",
    "            print(f\"  Mean: {np.mean(all_entropies):.2f}\")\n",
    "            print(f\"  Std: {np.std(all_entropies):.2f}\")\n",
    "            print(f\"  Min: {np.min(all_entropies):.2f}\")\n",
    "            print(f\"  Max: {np.max(all_entropies):.2f}\")\n",
    "\n",
    "def run_memory_efficient_analysis(csv_path: str, image_base_path: str,\n",
    "                                 output_dir: str, sample_size: int = 10):\n",
    "    \"\"\"\n",
    "    Main function to run memory-efficient MIMIC-CXR analysis\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"MEMORY-EFFICIENT MEDGEMMA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\nLoading MIMIC-CXR data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "\n",
    "    # Configure memory settings\n",
    "    memory_config = MemoryConfig(\n",
    "        max_new_tokens=20,  # Even more conservative\n",
    "        max_attention_tokens=5,  # Only analyze first 5 tokens\n",
    "        clear_cache_frequency=3  # Clear more frequently\n",
    "    )\n",
    "\n",
    "    # Initialize processor with automatic GPU selection\n",
    "    processor = MemoryEfficientMedGemmaProcessor(\n",
    "        memory_config=memory_config\n",
    "    )\n",
    "\n",
    "    # Process dataset\n",
    "    results_df = processor.process_dataset_batch(\n",
    "        df,\n",
    "        image_base_path,\n",
    "        output_dir,\n",
    "        sample_size=sample_size\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✓ Results saved to {output_dir}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Utility function to check GPU availability before running\n",
    "def check_gpu_availability():\n",
    "    \"\"\"Check and report GPU availability\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"GPU AVAILABILITY CHECK\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"❌ CUDA is not available!\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        import GPUtil\n",
    "        gpus = GPUtil.getGPUs()\n",
    "\n",
    "        suitable_gpus = []\n",
    "        for gpu in gpus:\n",
    "            free_gb = gpu.memoryFree / 1024\n",
    "            if free_gb >= 15:  # At least 15GB free\n",
    "                suitable_gpus.append((gpu.id, free_gb))\n",
    "\n",
    "        if suitable_gpus:\n",
    "            print(f\"✓ Found {len(suitable_gpus)} suitable GPU(s):\")\n",
    "            for gpu_id, free_mem in suitable_gpus:\n",
    "                print(f\"  GPU {gpu_id}: {free_mem:.1f}GB free\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ No GPUs with sufficient free memory (need at least 15GB)\")\n",
    "            return False\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"⚠️ GPUtil not installed. Install with: pip install gputil\")\n",
    "        print(\"Checking basic CUDA availability...\")\n",
    "        print(f\"✓ CUDA available with {torch.cuda.device_count()} GPU(s)\")\n",
    "        return True"
   ],
   "id": "e57b99386d2001d6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T22:08:51.885458Z",
     "start_time": "2025-08-10T22:08:30.779533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_path = \"/home/bsada1/lvlm-interpret-medgemma/one-pixel-attack/mimic_adapted_questions.csv\"\n",
    "image_base_path = \"/home/bsada1/mimic_cxr_hundred_vqa\"\n",
    "output_dir = \"mimic_medgemma_analysis\"\n",
    "results = run_memory_efficient_analysis(\n",
    "        csv_path,\n",
    "        image_base_path,\n",
    "        output_dir,\n",
    "        sample_size=10  # Start small\n",
    "    )"
   ],
   "id": "9177d6e24890ce82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MEMORY-EFFICIENT MEDGEMMA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Loading MIMIC-CXR data...\n",
      "Loaded 100 samples\n",
      "\n",
      "=== GPU Memory Status ===\n",
      "GPU 0: 10.2GB free / 80.0GB total (86.3% used)\n",
      "GPU 1: 68.9GB free / 80.0GB total (13.0% used)\n",
      "GPU 2: 78.4GB free / 80.0GB total (1.1% used)\n",
      "GPU 3: 78.4GB free / 80.0GB total (1.1% used)\n",
      "GPU 4: 78.4GB free / 80.0GB total (1.1% used)\n",
      "GPU 5: 78.4GB free / 80.0GB total (1.1% used)\n",
      "GPU 6: 78.4GB free / 80.0GB total (1.1% used)\n",
      "GPU 7: 78.4GB free / 80.0GB total (1.1% used)\n",
      "\n",
      "✓ Selected GPU 2 with 78.4GB free memory\n",
      "\n",
      "Initializing MedGemma on cuda:2\n",
      "Loading processor...\n",
      "Loading model with memory optimizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "GPU Memory: 8.0GB allocated, 8.0GB reserved\n",
      "\n",
      "Processing 10 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_139578/2408007231.py:163: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.memory_config.use_amp):\n",
      "Processing:  30%|███       | 3/10 [00:04<00:11,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Memory cleanup at sample 3]\n",
      "GPU Memory: 8.0GB allocated, 8.0GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 6/10 [00:09<00:06,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Memory cleanup at sample 6]\n",
      "GPU Memory: 8.0GB allocated, 8.0GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 9/10 [00:14<00:01,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Memory cleanup at sample 9]\n",
      "GPU Memory: 8.0GB allocated, 8.0GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10/10 [00:15<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Successful: 10/10\n",
      "Failed: 0\n",
      "Accuracy: 60.00%\n",
      "\n",
      "=== Attention Pattern Analysis ===\n",
      "\n",
      "Regional Focus Distribution:\n",
      "  upper_left: 100.0%\n",
      "\n",
      "Attention Entropy Statistics:\n",
      "  Mean: 4.27\n",
      "  Std: 0.15\n",
      "  Min: 4.01\n",
      "  Max: 4.74\n",
      "\n",
      "✓ Results saved to mimic_medgemma_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
