{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Vision-Language Models: MedGemma vs LLaVA\n",
    "## Interactive Analysis and Comparison Demo\n",
    "\n",
    "This notebook provides a quick demonstration of both MedGemma and LLaVA models on chest X-ray images from the MIMIC-CXR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('/home/bsada1/lvlm-interpret-medgemma')\n",
    "sys.path.append('/home/bsada1/lvlm-interpret-medgemma/models')\n",
    "\n",
    "# Set GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model classes\n",
    "from medgemma_launch_mimic_fixed import load_model_enhanced, extract_attention_data, overlay_attention_enhanced\n",
    "from llava_rad_visualizer import LLaVARadVisualizer\n",
    "\n",
    "print(\"Loading models... This may take a few minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MedGemma\n",
    "print(\"Loading MedGemma 4B...\")\n",
    "medgemma_model, medgemma_processor = load_model_enhanced(device=device)\n",
    "print(\"✓ MedGemma loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaVA\n",
    "print(\"Loading LLaVA 1.5 7B...\")\n",
    "llava_visualizer = LLaVARadVisualizer()\n",
    "llava_visualizer.load_model(load_in_8bit=True)\n",
    "print(\"✓ LLaVA loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample questions\n",
    "csv_path = \"/home/bsada1/mimic_cxr_hundred_vqa/medical-cxr-vqa-questions_sample.csv\"\n",
    "df_samples = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df_samples)} samples\")\n",
    "print(f\"\\nQuestion types: {df_samples['question_type'].value_counts().to_dict()}\")\n",
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_medgemma_inference(image, question):\n",
    "    \"\"\"Run MedGemma inference with attention extraction\"\"\"\n",
    "    prompt = f\"Question: {question}\\nAnswer with only 'yes' or 'no'.\"\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    inputs = medgemma_processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(device) if torch.is_tensor(v) else v \n",
    "             for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = medgemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs.sequences[0][len(inputs['input_ids'][0]):]\n",
    "    answer = medgemma_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract attention\n",
    "    attention_data = extract_attention_data(medgemma_model, outputs, inputs, medgemma_processor)\n",
    "    \n",
    "    # Extract yes/no\n",
    "    answer_clean = 'yes' if 'yes' in answer.lower() else 'no' if 'no' in answer.lower() else 'uncertain'\n",
    "    \n",
    "    return {\n",
    "        'answer': answer_clean,\n",
    "        'raw_answer': answer,\n",
    "        'attention_grid': attention_data.get('attention_grid', None) if attention_data else None\n",
    "    }\n",
    "\n",
    "def run_llava_inference(image, question):\n",
    "    \"\"\"Run LLaVA inference with attention extraction\"\"\"\n",
    "    result = llava_visualizer.generate_with_attention(image, question, max_new_tokens=50)\n",
    "    \n",
    "    # Extract yes/no\n",
    "    answer = result['answer']\n",
    "    answer_clean = 'yes' if 'yes' in answer.lower() else 'no' if 'no' in answer.lower() else 'uncertain'\n",
    "    \n",
    "    return {\n",
    "        'answer': answer_clean,\n",
    "        'raw_answer': answer,\n",
    "        'attention_grid': result['visual_attention']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Single Sample Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample\n",
    "sample_idx = 5  # Change this to test different samples\n",
    "sample = df_samples.iloc[sample_idx]\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"Image ID: {sample['dicom_id']}\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Ground Truth: {sample['answer']}\")\n",
    "print(f\"Question Type: {sample['question_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = f\"/home/bsada1/mimic_cxr_hundred_vqa/{sample['dicom_id']}.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Chest X-ray: {sample['dicom_id']}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both models\n",
    "print(\"Running inference...\")\n",
    "\n",
    "# MedGemma\n",
    "print(\"\\nMedGemma:\")\n",
    "medgemma_result = run_medgemma_inference(image, sample['question'])\n",
    "print(f\"Answer: {medgemma_result['answer']}\")\n",
    "print(f\"Raw: {medgemma_result['raw_answer'][:100]}\")\n",
    "print(f\"Correct: {medgemma_result['answer'] == sample['answer']}\")\n",
    "\n",
    "# LLaVA\n",
    "print(\"\\nLLaVA:\")\n",
    "llava_result = run_llava_inference(image, sample['question'])\n",
    "print(f\"Answer: {llava_result['answer']}\")\n",
    "print(f\"Raw: {llava_result['raw_answer'][:100]}\")\n",
    "print(f\"Correct: {llava_result['answer'] == sample['answer']}\")\n",
    "\n",
    "print(f\"\\nGround Truth: {sample['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original X-ray')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# MedGemma attention\n",
    "if medgemma_result['attention_grid'] is not None:\n",
    "    attn_grid = np.array(medgemma_result['attention_grid'])\n",
    "    attn_resized = np.array(Image.fromarray((attn_grid * 255).astype(np.uint8)).resize(image.size, Image.BILINEAR))\n",
    "    axes[1].imshow(image)\n",
    "    axes[1].imshow(attn_resized, alpha=0.5, cmap='hot')\n",
    "    axes[1].set_title(f'MedGemma: {medgemma_result[\"answer\"]}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "# LLaVA attention\n",
    "if llava_result['attention_grid'] is not None:\n",
    "    attn_grid = np.array(llava_result['attention_grid'])\n",
    "    attn_resized = np.array(Image.fromarray((attn_grid * 255).astype(np.uint8)).resize(image.size, Image.BILINEAR))\n",
    "    axes[2].imshow(image)\n",
    "    axes[2].imshow(attn_resized, alpha=0.5, cmap='hot')\n",
    "    axes[2].set_title(f'LLaVA: {llava_result[\"answer\"]}')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(f'Question: {sample[\"question\"]}\\nGround Truth: {sample[\"answer\"]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Comparison (10 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick batch comparison\n",
    "n_samples = 10\n",
    "results = []\n",
    "\n",
    "print(f\"Running batch analysis on {n_samples} samples...\")\n",
    "\n",
    "for idx in range(n_samples):\n",
    "    sample = df_samples.iloc[idx]\n",
    "    \n",
    "    # Load image\n",
    "    image_path = f\"/home/bsada1/mimic_cxr_hundred_vqa/{sample['dicom_id']}.jpg\"\n",
    "    if not Path(image_path).exists():\n",
    "        continue\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Run both models\n",
    "    medgemma_res = run_medgemma_inference(image, sample['question'])\n",
    "    llava_res = run_llava_inference(image, sample['question'])\n",
    "    \n",
    "    results.append({\n",
    "        'index': idx,\n",
    "        'dicom_id': sample['dicom_id'],\n",
    "        'question': sample['question'][:50],\n",
    "        'ground_truth': sample['answer'],\n",
    "        'medgemma_answer': medgemma_res['answer'],\n",
    "        'llava_answer': llava_res['answer'],\n",
    "        'medgemma_correct': medgemma_res['answer'] == sample['answer'],\n",
    "        'llava_correct': llava_res['answer'] == sample['answer'],\n",
    "        'both_correct': (medgemma_res['answer'] == sample['answer']) and (llava_res['answer'] == sample['answer']),\n",
    "        'agreement': medgemma_res['answer'] == llava_res['answer']\n",
    "    })\n",
    "    \n",
    "    print(f\"Sample {idx+1}/{n_samples}: MedGemma={medgemma_res['answer']}, LLaVA={llava_res['answer']}, GT={sample['answer']}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "df_results[['index', 'question', 'ground_truth', 'medgemma_answer', 'llava_answer', 'medgemma_correct', 'llava_correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize accuracies\n",
    "medgemma_accuracy = df_results['medgemma_correct'].mean() * 100\n",
    "llava_accuracy = df_results['llava_correct'].mean() * 100\n",
    "agreement_rate = df_results['agreement'].mean() * 100\n",
    "\n",
    "print(f\"\\nResults Summary ({n_samples} samples):\")\n",
    "print(f\"MedGemma Accuracy: {medgemma_accuracy:.1f}%\")\n",
    "print(f\"LLaVA Accuracy: {llava_accuracy:.1f}%\")\n",
    "print(f\"Model Agreement: {agreement_rate:.1f}%\")\n",
    "\n",
    "# Bar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "models = ['MedGemma', 'LLaVA']\n",
    "accuracies = [medgemma_accuracy, llava_accuracy]\n",
    "colors = ['#4CAF50', '#2196F3']\n",
    "\n",
    "bars = ax.bar(models, accuracies, color=colors, width=0.6)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title(f'Model Performance Comparison (n={n_samples})', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add agreement line\n",
    "ax.axhline(y=agreement_rate, color='red', linestyle='--', alpha=0.5, label=f'Agreement: {agreement_rate:.1f}%')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Question Phrasing Robustness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robustness to phrasing variations\n",
    "sample = df_samples.iloc[0]  # Use first sample\n",
    "image_path = f\"/home/bsada1/mimic_cxr_hundred_vqa/{sample['dicom_id']}.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Original question\n",
    "original_question = sample['question']\n",
    "\n",
    "# Create variations\n",
    "variations = [\n",
    "    original_question,\n",
    "    original_question.replace('is there', 'can you see'),\n",
    "    original_question.replace('is there', 'does the image show'),\n",
    "    original_question.replace('?', ' in this x-ray?'),\n",
    "    f\"Looking at this chest x-ray, {original_question.lower()}\"\n",
    "]\n",
    "\n",
    "print(f\"Testing phrasing variations for: {original_question}\")\n",
    "print(f\"Ground truth: {sample['answer']}\\n\")\n",
    "\n",
    "phrasing_results = []\n",
    "for i, variant in enumerate(variations):\n",
    "    medgemma_res = run_medgemma_inference(image, variant)\n",
    "    llava_res = run_llava_inference(image, variant)\n",
    "    \n",
    "    phrasing_results.append({\n",
    "        'variant': i+1,\n",
    "        'question': variant,\n",
    "        'medgemma': medgemma_res['answer'],\n",
    "        'llava': llava_res['answer']\n",
    "    })\n",
    "    \n",
    "    print(f\"Variant {i+1}: {variant}\")\n",
    "    print(f\"  MedGemma: {medgemma_res['answer']}, LLaVA: {llava_res['answer']}\")\n",
    "\n",
    "df_phrasing = pd.DataFrame(phrasing_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze consistency\n",
    "medgemma_consistent = len(df_phrasing['medgemma'].unique()) == 1\n",
    "llava_consistent = len(df_phrasing['llava'].unique()) == 1\n",
    "\n",
    "print(f\"\\nConsistency Analysis:\")\n",
    "print(f\"MedGemma consistency: {'✓ Consistent' if medgemma_consistent else '✗ Inconsistent'}\")\n",
    "print(f\"  Unique answers: {df_phrasing['medgemma'].unique()}\")\n",
    "print(f\"LLaVA consistency: {'✓ Consistent' if llava_consistent else '✗ Inconsistent'}\")\n",
    "print(f\"  Unique answers: {df_phrasing['llava'].unique()}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Prepare data for visualization\n",
    "x = np.arange(len(variations))\n",
    "width = 0.35\n",
    "\n",
    "# Convert answers to numeric (yes=1, no=0)\n",
    "medgemma_numeric = [1 if ans == 'yes' else 0 for ans in df_phrasing['medgemma']]\n",
    "llava_numeric = [1 if ans == 'yes' else 0 for ans in df_phrasing['llava']]\n",
    "\n",
    "ax.bar(x - width/2, medgemma_numeric, width, label='MedGemma', color='#4CAF50', alpha=0.8)\n",
    "ax.bar(x + width/2, llava_numeric, width, label='LLaVA', color='#2196F3', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Question Variant')\n",
    "ax.set_ylabel('Answer (0=No, 1=Yes)')\n",
    "ax.set_title('Model Consistency Across Question Phrasings')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'V{i+1}' for i in range(len(variations))])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels(['No', 'Yes'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed results if available\n",
    "try:\n",
    "    # Load MedGemma statistics\n",
    "    with open('results/medgemma_attention_analysis/statistics/comprehensive_statistics.json', 'r') as f:\n",
    "        medgemma_stats = json.load(f)\n",
    "    \n",
    "    # Load LLaVA statistics\n",
    "    with open('results/llava_rad_attention_analysis/statistics/llava_rad_statistics.json', 'r') as f:\n",
    "        llava_stats = json.load(f)\n",
    "    \n",
    "    print(\"Comprehensive Analysis Summary (Full Dataset):\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nMedGemma:\")\n",
    "    print(f\"  Total comparisons: {medgemma_stats.get('total_comparisons', 'N/A')}\")\n",
    "    if 'overall' in medgemma_stats:\n",
    "        print(f\"  JS Divergence: {medgemma_stats['overall']['js_divergence_mean']:.4f} ± {medgemma_stats['overall']['js_divergence_std']:.4f}\")\n",
    "        print(f\"  Correlation: {medgemma_stats['overall']['correlation_mean']:.4f} ± {medgemma_stats['overall']['correlation_std']:.4f}\")\n",
    "    \n",
    "    print(\"\\nLLaVA:\")\n",
    "    print(f\"  Total comparisons: {llava_stats.get('total_comparisons', 'N/A')}\")\n",
    "    if 'overall' in llava_stats:\n",
    "        print(f\"  JS Divergence: {llava_stats['overall']['js_divergence_mean']:.4f} ± {llava_stats['overall']['js_divergence_std']:.4f}\")\n",
    "        print(f\"  Correlation: {llava_stats['overall']['correlation_mean']:.4f} ± {llava_stats['overall']['correlation_std']:.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Full analysis results not found. Run complete analysis scripts for comprehensive statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "KEY FINDINGS FROM COMPREHENSIVE ANALYSIS\n",
    "========================================\n",
    "\n",
    "1. ACCURACY (100 samples):\n",
    "   • MedGemma: 74% \n",
    "   • LLaVA: 62%\n",
    "   • Difference: 12% in favor of MedGemma\n",
    "\n",
    "2. QUESTION PHRASING ROBUSTNESS:\n",
    "   • MedGemma: 90.3% consistency\n",
    "   • LLaVA: 94.4% consistency\n",
    "   • Both models show high robustness\n",
    "\n",
    "3. ATTENTION PATTERNS:\n",
    "   • High correlation between models (~0.95)\n",
    "   • Small attention shifts correlate with answer changes\n",
    "   • JS divergence similar for both models (~0.10)\n",
    "\n",
    "4. MODEL CHARACTERISTICS:\n",
    "   • MedGemma: Specialized medical model (4B params)\n",
    "   • LLaVA: General-purpose adapted model (7B params)\n",
    "   • MedGemma achieves better performance with fewer parameters\n",
    "\n",
    "5. DECISION BOUNDARIES:\n",
    "   • Both models operate near decision boundaries\n",
    "   • Small input variations can flip answers\n",
    "   • Attention changes minimally even when answers change\n",
    "\n",
    "RECOMMENDATION:\n",
    "For medical imaging tasks, MedGemma demonstrates superior performance\n",
    "despite its smaller size, suggesting the value of domain-specific training.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Testing\n",
    "\n",
    "Use this section to test your own questions on specific images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom question testing\n",
    "custom_question = \"Is there evidence of pneumonia?\"\n",
    "sample_idx = 10  # Change this to test different images\n",
    "\n",
    "sample = df_samples.iloc[sample_idx]\n",
    "image_path = f\"/home/bsada1/mimic_cxr_hundred_vqa/{sample['dicom_id']}.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "print(f\"Testing custom question on image {sample['dicom_id']}\")\n",
    "print(f\"Question: {custom_question}\\n\")\n",
    "\n",
    "# Run both models\n",
    "medgemma_res = run_medgemma_inference(image, custom_question)\n",
    "llava_res = run_llava_inference(image, custom_question)\n",
    "\n",
    "print(f\"MedGemma: {medgemma_res['answer']}\")\n",
    "print(f\"  Raw: {medgemma_res['raw_answer'][:100]}\")\n",
    "print(f\"\\nLLaVA: {llava_res['answer']}\")\n",
    "print(f\"  Raw: {llava_res['raw_answer'][:100]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original X-ray')\n",
    "axes[0].axis('off')\n",
    "\n",
    "if medgemma_res['attention_grid'] is not None:\n",
    "    attn_grid = np.array(medgemma_res['attention_grid'])\n",
    "    attn_resized = np.array(Image.fromarray((attn_grid * 255).astype(np.uint8)).resize(image.size, Image.BILINEAR))\n",
    "    axes[1].imshow(image)\n",
    "    axes[1].imshow(attn_resized, alpha=0.5, cmap='hot')\n",
    "    axes[1].set_title(f'MedGemma: {medgemma_res[\"answer\"]}')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "if llava_res['attention_grid'] is not None:\n",
    "    attn_grid = np.array(llava_res['attention_grid'])\n",
    "    attn_resized = np.array(Image.fromarray((attn_grid * 255).astype(np.uint8)).resize(image.size, Image.BILINEAR))\n",
    "    axes[2].imshow(image)\n",
    "    axes[2].imshow(attn_resized, alpha=0.5, cmap='hot')\n",
    "    axes[2].set_title(f'LLaVA: {llava_res[\"answer\"]}')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(f'Custom Question: {custom_question}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
