{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyOwjPOeWh3Yivw+j1STAjlC",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "ed0bd465333b46b6a0c2729c8d1867fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5a73712ca4441d88f83694ec7ab92b7",
       "IPY_MODEL_1ad65a8a4e8e49518d28cd4ea368d3dd",
       "IPY_MODEL_7f84911b6b4c43c2a3a272fda8ef94ec"
      ],
      "layout": "IPY_MODEL_37d6c0fafeaf46a895bb45e80fd58fd2"
     }
    },
    "f5a73712ca4441d88f83694ec7ab92b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ab143236b854e9dab1502fa1ebc67af",
      "placeholder": "​",
      "style": "IPY_MODEL_b9996dd666eb406a8518d5842ceca147",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "1ad65a8a4e8e49518d28cd4ea368d3dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_049cb4e8874e4ba6bb8605e1408e907e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ea8d9ee56d14c189e571e4a60dfaec0",
      "value": 2
     }
    },
    "7f84911b6b4c43c2a3a272fda8ef94ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4e52a110b14e5498bffe6868aef52b",
      "placeholder": "​",
      "style": "IPY_MODEL_0a543adeabec4cf2b6278383d3602dec",
      "value": " 2/2 [00:02&lt;00:00,  1.39s/it]"
     }
    },
    "37d6c0fafeaf46a895bb45e80fd58fd2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ab143236b854e9dab1502fa1ebc67af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9996dd666eb406a8518d5842ceca147": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "049cb4e8874e4ba6bb8605e1408e907e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ea8d9ee56d14c189e571e4a60dfaec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d4e52a110b14e5498bffe6868aef52b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a543adeabec4cf2b6278383d3602dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/UNHSAILLab/lvlm-interpret-medgemma/blob/main/MedGemma_Attention_Visualization_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import logging\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import gc\n",
    "# Disable parallelism to avoid conflicts\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "id": "FmRG35PBOqNn"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Fresh start initialized\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X13s2HzOs96",
    "outputId": "65d35548-5260-47f6-8e4c-cf78002bce3d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Clear everything\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=== GPU-Safe Setup ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmtnrO06O_L8",
    "outputId": "2e60a7d2-672a-42f5-93da-cfd771ffa94a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Loading Model with Safe Config ===\")\n",
    "\n",
    "model_id='google/medgemma-4b-it'\n",
    "\n",
    "# Load processor first\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "print(\"✓ Processor loaded\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJH7f-w6PCa0",
    "outputId": "3381daf2-0701-4d67-eaff-d0b9b7da116d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,  # Use bfloat16 instead of float16\n",
    "        device_map=\"cuda:0\",\n",
    "        attn_implementation=\"eager\",\n",
    "        # Important: set these to avoid issues\n",
    "        tie_word_embeddings=False\n",
    "    )\n",
    "\n",
    "    # Critical: set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure attention output is enabled\n",
    "    model.config.output_attentions = True\n",
    "\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    raise"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "ed0bd465333b46b6a0c2729c8d1867fd",
      "f5a73712ca4441d88f83694ec7ab92b7",
      "1ad65a8a4e8e49518d28cd4ea368d3dd",
      "7f84911b6b4c43c2a3a272fda8ef94ec",
      "37d6c0fafeaf46a895bb45e80fd58fd2",
      "5ab143236b854e9dab1502fa1ebc67af",
      "b9996dd666eb406a8518d5842ceca147",
      "049cb4e8874e4ba6bb8605e1408e907e",
      "2ea8d9ee56d14c189e571e4a60dfaec0",
      "8d4e52a110b14e5498bffe6868aef52b",
      "0a543adeabec4cf2b6278383d3602dec"
     ]
    },
    "id": "fFTciWNSPKuo",
    "outputId": "63ffa924-1707-45fe-efff-ab5cbe05d0e9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "image_path = \"/content/sample-xray.jpg\"\n",
    "try:\n",
    "    xray_pil_image = Image.open(image_path)\n",
    "    print(f\"✓ Successfully loaded image: {image_path}\")\n",
    "    print(f\"  Image format: {xray_pil_image.format}\")\n",
    "    print(f\"  Image size: {xray_pil_image.size}\")\n",
    "    print(f\"  Image mode: {xray_pil_image.mode}\")\n",
    "\n",
    "    # Optionally convert to RGB if it's not already\n",
    "    if xray_pil_image.mode != 'RGB':\n",
    "        xray_pil_image = xray_pil_image.convert('RGB')\n",
    "        print(\"  Converted image mode to RGB.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: Image file not found at {image_path}\")\n",
    "    xray_pil_image = None # Set to None to indicate failure\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading image: {e}\")\n",
    "    xray_pil_image = None\n",
    "\n",
    "if xray_pil_image is not None:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(xray_pil_image)\n",
    "    plt.title(\"Loaded Chest X-ray (PIL)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Image could not be loaded for further processing.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "bLCTr2gNPQ6o",
    "outputId": "4f97b260-1bb9-4462-ae0f-b7418c259087"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Preparing Inputs ===\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image\", \"image\": image_path}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move to GPU and verify\n",
    "inputs_gpu = {}\n",
    "for k, v in inputs.items():\n",
    "    if torch.is_tensor(v):\n",
    "        inputs_gpu[k] = v.to(\"cuda:0\")\n",
    "        print(f\"{k}: shape={v.shape}, device=cuda:0\")\n",
    "        # Check for invalid values\n",
    "        if k == \"input_ids\":\n",
    "            max_id = v.max().item()\n",
    "            min_id = v.min().item()\n",
    "            print(f\"  ID range: [{min_id}, {max_id}]\")\n",
    "            vocab_size = processor.tokenizer.vocab_size\n",
    "            print(f\"  Vocab size: {vocab_size}\")\n",
    "            if max_id >= vocab_size:\n",
    "                print(f\"  ⚠️ WARNING: Token ID {max_id} >= vocab size {vocab_size}\")\n",
    "    else:\n",
    "        inputs_gpu[k] = v\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI63MkUiPdRW",
    "outputId": "e617b3be-09f3-4c62-e869-6461715989d3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Testing Forward Pass ===\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Just forward pass, no generation\n",
    "        outputs = model(\n",
    "            **inputs_gpu,\n",
    "            output_attentions=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "    print(\"✓ Forward pass successful\")\n",
    "\n",
    "    # Check outputs\n",
    "    if hasattr(outputs, 'attentions') and outputs.attentions:\n",
    "        print(f\"✓ Attentions available: {len(outputs.attentions)} layers\")\n",
    "        print(f\"  First layer shape: {outputs.attentions[0].shape}\")\n",
    "    else:\n",
    "        print(\"❌ No attentions in forward pass\")\n",
    "\n",
    "    # Check logits\n",
    "    if hasattr(outputs, 'logits'):\n",
    "        print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "        # Check for NaN/Inf\n",
    "        has_nan = torch.isnan(outputs.logits).any()\n",
    "        has_inf = torch.isinf(outputs.logits).any()\n",
    "        print(f\"Logits has NaN: {has_nan}, has Inf: {has_inf}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Forward pass failed: {e}\")\n",
    "    raise\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dv_O3FwsPhgm",
    "outputId": "683a3104-91a0-4b22-d942-14545ad9bb5c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"\\n=== Medical Region Analysis ===\")\n",
    "\n",
    "# Aggregate attention across all generated tokens\n",
    "all_attention_to_input = []\n",
    "for token_idx in range(len(attention_data)):\n",
    "    token_attn = attention_data[token_idx][-1].cpu().float()  # Last layer\n",
    "    if len(token_attn.shape) == 4:\n",
    "        token_attn = token_attn[0]  # Remove batch\n",
    "    # Average over heads\n",
    "    avg_token_attn = token_attn.mean(dim=0)\n",
    "    # Get attention from generated position to inputs\n",
    "    gen_pos = len(inputs['input_ids'][0]) + token_idx\n",
    "    if gen_pos < avg_token_attn.shape[0]:\n",
    "        all_attention_to_input.append(avg_token_attn[gen_pos, :len(inputs['input_ids'][0])])\n",
    "\n",
    "# Average across all generated tokens\n",
    "if all_attention_to_input:\n",
    "    avg_attention_to_input = torch.stack(all_attention_to_input).mean(dim=0)\n",
    "else:\n",
    "    avg_attention_to_input = gen_to_input_attn[:len(inputs['input_ids'][0])]\n",
    "\n",
    "# Create final visualization\n",
    "create_attention_overlay(\n",
    "    xray_pil_image,\n",
    "    avg_attention_to_input,\n",
    "    \"Average Attention from Generated Medical Report\"\n",
    ")\n",
    "\n",
    "# Identify key phrases in the generated report\n",
    "print(f\"\\n=== Generated Medical Report ===\")\n",
    "print(medical_report)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'medical_report': medical_report,\n",
    "    'attention_data': attention_data,\n",
    "    'input_length': len(inputs['input_ids'][0]),\n",
    "    'avg_attention_to_input': avg_attention_to_input,\n",
    "    'high_attention_positions': high_attn_positions\n",
    "}\n",
    "\n",
    "torch.save(results, 'chest_xray_attention_results.pt')\n",
    "print(\"\\n✓ Saved chest X-ray attention analysis\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "hcYCncPlRcsz",
    "outputId": "8e9681af-051f-4eae-fa23-7eeadb243b76"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Safe Generation (Greedy) ===\")\n",
    "\n",
    "# Configure for safest possible generation\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 25,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"do_sample\": False,  # CRITICAL: No sampling to avoid multinomial errors\n",
    "    \"num_beams\": 1,      # No beam search\n",
    "    \"temperature\": 1.0,  # Doesn't matter with do_sample=False\n",
    "    \"output_attentions\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"pad_token_id\": processor.tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "    \"use_cache\": True\n",
    "}\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs_gpu,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "    print(\"✓ Generation successful!\")\n",
    "\n",
    "    # Decode output\n",
    "    generated_ids = outputs.sequences[0][len(inputs['input_ids'][0]):]\n",
    "    generated_text = processor.decode(generated_ids, skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated: {generated_text}\")\n",
    "\n",
    "    # Check for attentions\n",
    "    attention_found = False\n",
    "    if hasattr(outputs, 'attentions') and outputs.attentions:\n",
    "        print(f\"\\n✓ Attentions captured!\")\n",
    "        print(f\"  Structure: {len(outputs.attentions)} tokens\")\n",
    "        if len(outputs.attentions) > 0:\n",
    "            print(f\"  Layers per token: {len(outputs.attentions[0])}\")\n",
    "            if len(outputs.attentions[0]) > 0:\n",
    "                print(f\"  Shape: {outputs.attentions[0][0].shape}\")\n",
    "        attention_found = True\n",
    "        attention_data = outputs.attentions\n",
    "\n",
    "    # Also check other possible locations\n",
    "    for attr in ['decoder_attentions', 'encoder_attentions', 'cross_attentions']:\n",
    "        if hasattr(outputs, attr) and getattr(outputs, attr) is not None:\n",
    "            print(f\"  Also found: {attr}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Generation failed: {e}\")\n",
    "    print(\"\\nError details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    attention_found = False\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-WKR41EPzfV",
    "outputId": "a3b9d7a0-1663-4d0d-aac6-90d85b2ff60c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if attention_found and attention_data:\n",
    "    print(\"\\n=== Visualizing Attention ===\")\n",
    "\n",
    "    # Get first token's last layer attention\n",
    "    first_token_attn = attention_data[0][-1]  # First token, last layer\n",
    "\n",
    "    # Move to CPU and convert to float32 for visualization\n",
    "    if first_token_attn.is_cuda:\n",
    "        first_token_attn = first_token_attn.cpu()\n",
    "\n",
    "    # CRITICAL: Convert from bfloat16 to float32\n",
    "    first_token_attn = first_token_attn.float()\n",
    "\n",
    "    print(f\"Attention tensor shape: {first_token_attn.shape}\")\n",
    "    print(f\"Attention tensor dtype: {first_token_attn.dtype}\")\n",
    "\n",
    "    # Plot based on shape\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if len(first_token_attn.shape) == 4:\n",
    "        # [batch, heads, seq, seq]\n",
    "        # Show first 4 heads\n",
    "        for i in range(min(4, first_token_attn.shape[1])):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            attn_matrix = first_token_attn[0, i].numpy()\n",
    "            plt.imshow(attn_matrix, cmap='hot', aspect='auto')\n",
    "            plt.colorbar()\n",
    "            plt.title(f'Head {i}')\n",
    "            plt.xlabel('Keys')\n",
    "            plt.ylabel('Queries')\n",
    "    else:\n",
    "        # Single plot\n",
    "        if len(first_token_attn.shape) == 3:\n",
    "            plot_data = first_token_attn[0]  # First head\n",
    "        else:\n",
    "            plot_data = first_token_attn\n",
    "\n",
    "        plt.imshow(plot_data.numpy(), cmap='hot', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title('Attention Matrix')\n",
    "        plt.xlabel('Keys')\n",
    "        plt.ylabel('Queries')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze attention patterns\n",
    "    print(\"\\n=== Attention Analysis ===\")\n",
    "    print(f\"Sequence length: 273\")\n",
    "    print(f\"Number of heads: {first_token_attn.shape[1]}\")\n",
    "\n",
    "    # Get average attention across heads\n",
    "    avg_attention = first_token_attn[0].mean(dim=0)  # Average over heads\n",
    "\n",
    "    # Look at attention from last position (generated token)\n",
    "    last_pos_attention = avg_attention[-1, :]\n",
    "\n",
    "    # Find top attended positions\n",
    "    top_k = 10\n",
    "    top_values, top_indices = torch.topk(last_pos_attention, k=min(top_k, len(last_pos_attention)))\n",
    "\n",
    "    print(f\"\\nTop {len(top_values)} attended positions from generated token:\")\n",
    "    for i, (val, idx) in enumerate(zip(top_values, top_indices)):\n",
    "        print(f\"  {i+1}. Position {idx}: {val:.4f}\")\n",
    "\n",
    "    # Plot attention distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(last_pos_attention.numpy())\n",
    "    plt.title('Attention Distribution from Generated Token')\n",
    "    plt.xlabel('Input Position')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(avg_attention.numpy(), cmap='hot', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title('Average Attention Matrix (All Heads)')\n",
    "    plt.xlabel('Key Positions')\n",
    "    plt.ylabel('Query Positions')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save attention data for chest X-ray analysis\n",
    "    save_data = {\n",
    "        'attention_data': attention_data,\n",
    "        'input_ids': inputs_gpu['input_ids'].cpu(),\n",
    "        'input_length': len(inputs['input_ids'][0]),\n",
    "        'generated_text': generated_text,\n",
    "        'model_name': model_name,\n",
    "        'sequence_length': 273,\n",
    "        'num_heads': first_token_attn.shape[1],\n",
    "        'num_layers': len(attention_data[0])\n",
    "    }\n",
    "\n",
    "    torch.save(save_data, 'medgemma_attention_extracted.pt')\n",
    "    print(\"\\n✓ Saved attention data for analysis\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS! Attention extraction complete.\")\n",
    "    print(f\"- Sequence length: 273 tokens\")\n",
    "    print(f\"- Number of heads: {first_token_attn.shape[1]}\")\n",
    "    print(f\"- Number of layers: {len(attention_data[0])}\")\n",
    "    print(\"\\nReady for chest X-ray specific visualization!\")\n",
    "    print(\"=\"*50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wfgA0Q47P7ET",
    "outputId": "d632b278-3d49-44c9-821f-f2204ac60c3f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPU-Safe Extraction Summary:\")\n",
    "print(f\"- Model loaded: ✓\")\n",
    "print(f\"- Forward pass works: {'✓' if 'outputs' in locals() else '✗'}\")\n",
    "print(f\"- Generation works: {'✓' if attention_found else '✗'}\")\n",
    "print(f\"- Attention captured: {'✓' if attention_found else '✗'}\")\n",
    "\n",
    "if attention_found:\n",
    "    print(\"\\n✓ SUCCESS! Ready for chest X-ray analysis.\")\n",
    "    print(\"\\nKey settings that work:\")\n",
    "    print(\"- dtype: bfloat16 (not float16)\")\n",
    "    print(\"- Greedy decoding (do_sample=False)\")\n",
    "    print(\"- attn_implementation='eager'\")\n",
    "else:\n",
    "    print(\"\\n⚠️ If still failing, try:\")\n",
    "    print(\"1. pip install --upgrade transformers accelerate\")\n",
    "    print(\"2. Try smaller model: google/medgemma-2b-it\")\n",
    "    print(\"3. Check GPU drivers: nvidia-smi\")\n",
    "\n",
    "print(\"=\"*50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rpp2viTvQ8UI",
    "outputId": "b4d9e62a-42c1-424e-b81c-82c50d6c7abc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Processing Chest X-ray ===\")\n",
    "\n",
    "# Medical prompt\n",
    "medical_prompt = \"\"\"Analyze this chest X-ray. Report on:\n",
    "1. Lung fields\n",
    "2. Heart size\n",
    "3. Any abnormalities\"\"\"\n",
    "\n",
    "# Create messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist. Respond in less than 150 tokens\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": medical_prompt},\n",
    "            {\"type\": \"image\", \"image\": xray_pil_image}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "inputs_gpu = {k: v.to(\"cuda:0\") if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
    "\n",
    "print(f\"Input length: {inputs_gpu['input_ids'].shape[1]} tokens\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwYmwrCHRFK2",
    "outputId": "c6db2e0c-e572-4da1-a732-fd28e22df96b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: Generate with attention capture\n",
    "print(\"\\n=== Generating Medical Report ===\")\n",
    "\n",
    "# Use the working configuration\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 150,  # Longer for medical description\n",
    "    \"do_sample\": False,\n",
    "    \"output_attentions\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"pad_token_id\": processor.tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs_gpu, **gen_kwargs)\n",
    "\n",
    "# Decode\n",
    "generated_ids = outputs.sequences[0][len(inputs['input_ids'][0]):]\n",
    "medical_report = processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nMedical Report:\\n{medical_report}\")\n",
    "\n",
    "# Get attention data\n",
    "attention_data = outputs.attentions\n",
    "print(f\"\\nCaptured attention for {len(attention_data)} tokens\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvZexrcORL21",
    "outputId": "262093ab-2c1f-4480-8ca4-e4624ec8c425"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: Analyze attention structure for image regions\n",
    "print(\"\\n=== Analyzing Image Attention ===\")\n",
    "\n",
    "# Get input tokens to understand structure\n",
    "input_ids = inputs_gpu['input_ids'][0].cpu()\n",
    "input_tokens = [processor.tokenizer.decode([id]) for id in input_ids]\n",
    "\n",
    "print(f\"Total input length: {len(input_tokens)}\")\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "for i, token in enumerate(input_tokens[:20]):\n",
    "    print(f\"  {i}: '{token}'\")\n",
    "\n",
    "# For MedGemma, image information is embedded in the sequence\n",
    "# We need to identify which positions correspond to image processing\n",
    "# This is model-specific and may require inspection\n",
    "\n",
    "# Let's analyze attention patterns to identify image regions\n",
    "# Get attention from first generated token to all inputs\n",
    "first_gen_attn = attention_data[0][-1]  # First generated token, last layer\n",
    "if first_gen_attn.is_cuda:\n",
    "    first_gen_attn = first_gen_attn.cpu().float()\n",
    "\n",
    "# Average over heads\n",
    "avg_attn = first_gen_attn[0].mean(dim=0)  # [seq_len, seq_len]\n",
    "gen_to_input_attn = avg_attn[-1, :]  # Attention from generated token to inputs\n",
    "\n",
    "# Find regions with high attention\n",
    "high_attn_threshold = gen_to_input_attn.quantile(0.9)\n",
    "high_attn_positions = torch.where(gen_to_input_attn > high_attn_threshold)[0]\n",
    "\n",
    "print(f\"\\nHigh attention positions: {high_attn_positions.tolist()}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARmhxgOURRe2",
    "outputId": "554489c0-2c48-4f9b-e4b2-3077fdbf817e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: Create attention heatmap for image regions\n",
    "print(\"\\n=== Creating Attention Visualizations ===\")\n",
    "\n",
    "# Function to create attention overlay\n",
    "def create_attention_overlay(chest_xray, attention_weights, title=\"Attention Heatmap\"):\n",
    "    \"\"\"Create an overlay of attention on chest X-ray\"\"\"\n",
    "\n",
    "    # Ensure attention is 2D\n",
    "    if len(attention_weights.shape) == 1:\n",
    "        # Reshape to approximate square\n",
    "        size = int(np.sqrt(len(attention_weights)))\n",
    "        if size * size < len(attention_weights):\n",
    "            size += 1\n",
    "        # Pad if necessary\n",
    "        padded = torch.zeros(size * size)\n",
    "        padded[:len(attention_weights)] = attention_weights\n",
    "        attention_2d = padded.reshape(size, size)\n",
    "    else:\n",
    "        attention_2d = attention_weights\n",
    "\n",
    "    # Resize attention to match image size\n",
    "    attention_np = attention_2d.numpy()\n",
    "    attention_resized = Image.fromarray((attention_np * 255).astype(np.uint8))\n",
    "    attention_resized = attention_resized.resize(chest_xray.size, Image.BICUBIC)\n",
    "\n",
    "    # Create colored overlay\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(chest_xray, cmap='gray')\n",
    "    plt.title(\"Original Chest X-ray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Attention heatmap\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(attention_np, cmap='hot', interpolation='nearest')\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Overlay\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(chest_xray, cmap='gray')\n",
    "    plt.imshow(attention_resized, cmap='hot', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for different layers and heads\n",
    "# Last layer, different heads\n",
    "last_layer_attn = attention_data[0][-1].cpu().float()[0]  # Remove batch dimension\n",
    "\n",
    "print(\"Attention from different heads:\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(min(8, last_layer_attn.shape[0])):\n",
    "    ax = axes[i//4, i%4]\n",
    "    # Get attention from last position (generated) to all positions\n",
    "    head_attn = last_layer_attn[i, -1, :]\n",
    "\n",
    "    # Reshape for visualization\n",
    "    size = int(np.sqrt(len(head_attn)))\n",
    "    if size * size < len(head_attn):\n",
    "        size += 1\n",
    "    padded = torch.zeros(size * size)\n",
    "    padded[:len(head_attn)] = head_attn\n",
    "    attn_2d = padded.reshape(size, size)\n",
    "\n",
    "    im = ax.imshow(attn_2d.numpy(), cmap='hot')\n",
    "    ax.set_title(f'Head {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "bt981jurRW2Y",
    "outputId": "669f9c6e-4a3d-411f-b70f-798e00b6e865"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7: Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Chest X-ray Attention Analysis Complete!\")\n",
    "print(f\"- Generated {len(medical_report.split())} words\")\n",
    "print(f\"- Analyzed {len(attention_data)} attention steps\")\n",
    "print(f\"- Identified {len(high_attn_positions)} high-attention positions\")\n",
    "print(\"\\nThe attention heatmaps show which parts of the image the model\")\n",
    "print(\"focused on when generating each part of the medical report.\")\n",
    "print(\"=\"*50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25u6M1edRgik",
    "outputId": "2c3fa63e-4772-4352-b88e-affcf21195a4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "De67TgaOXoiG",
    "outputId": "12661cdf-77b8-412c-aad3-cda53fe92aac"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_model_structure(model, processor, inputs):\n",
    "    \"\"\"Analyze model to understand token structure\"\"\"\n",
    "\n",
    "    # Get model config\n",
    "    config = model.config\n",
    "\n",
    "    # Vision encoder details (from your description)\n",
    "    vision_config = {\n",
    "        'patch_size': 14,\n",
    "        'image_size': 224,  # Typical for medical models\n",
    "        'num_patches': (224 // 14) ** 2,  # 16x16 = 256 patches\n",
    "        'vision_tokens': 256,  # After projection\n",
    "    }\n",
    "\n",
    "    # Decode some tokens to understand structure\n",
    "    input_ids = inputs['input_ids'][0].cpu()\n",
    "    tokens = [processor.tokenizer.decode([id]) for id in input_ids[:50]]\n",
    "\n",
    "    # Find special tokens\n",
    "    special_tokens = {\n",
    "        'bos': processor.tokenizer.bos_token_id,\n",
    "        'eos': processor.tokenizer.eos_token_id,\n",
    "        'pad': processor.tokenizer.pad_token_id,\n",
    "    }\n",
    "\n",
    "    # Identify image token positions\n",
    "    # For vision-language models, image tokens are usually:\n",
    "    # 1. After BOS token\n",
    "    # 2. Before the text prompt\n",
    "    # 3. Continuous block of tokens\n",
    "\n",
    "    image_start = 1  # After BOS\n",
    "    image_end = image_start + vision_config['vision_tokens']\n",
    "\n",
    "    return vision_config, image_start, image_end\n",
    "\n",
    "def extract_image_attention(attention_data, image_start, image_end, target_layer=-1):\n",
    "    \"\"\"Extract attention specifically for image tokens\"\"\"\n",
    "\n",
    "    # Get attention from specified layer\n",
    "    layer_attention = attention_data[0][target_layer].cpu().float()\n",
    "\n",
    "    if len(layer_attention.shape) == 4:\n",
    "        layer_attention = layer_attention[0]  # Remove batch\n",
    "\n",
    "    # Extract attention TO image tokens FROM generated tokens\n",
    "    # Shape: [num_heads, seq_len, seq_len]\n",
    "\n",
    "    # Get attention to image region\n",
    "    image_attention = layer_attention[:, :, image_start:image_end]\n",
    "\n",
    "    return image_attention\n",
    "\n",
    "def create_spatial_attention_map(attention_weights, vision_config):\n",
    "    \"\"\"Convert linear attention to spatial map\"\"\"\n",
    "\n",
    "    num_patches_per_side = int(np.sqrt(vision_config['vision_tokens']))\n",
    "\n",
    "    if len(attention_weights.shape) == 1:\n",
    "        # Single vector of attention weights\n",
    "        # Reshape to spatial grid\n",
    "        spatial_map = attention_weights.reshape(num_patches_per_side, num_patches_per_side)\n",
    "    else:\n",
    "        # Multiple positions attending to image\n",
    "        # Average across all query positions\n",
    "        spatial_map = attention_weights.mean(dim=0).reshape(num_patches_per_side, num_patches_per_side)\n",
    "\n",
    "    return spatial_map\n",
    "\n",
    "def visualize_attention_on_image(image, attention_map, vision_config, smooth=True):\n",
    "    \"\"\"Create high-quality attention overlay\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Original image\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title('Original Chest X-ray')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Attention heatmap\n",
    "    if smooth:\n",
    "        # Upsample attention map smoothly\n",
    "        attention_tensor = torch.tensor(attention_map).unsqueeze(0).unsqueeze(0)\n",
    "        upsampled = F.interpolate(attention_tensor,\n",
    "                                size=image.size[::-1],\n",
    "                                mode='bicubic',\n",
    "                                align_corners=False)\n",
    "        attention_map_vis = upsampled.squeeze().numpy()\n",
    "    else:\n",
    "        attention_map_vis = np.array(Image.fromarray((attention_map * 255).astype(np.uint8))\n",
    "                                   .resize(image.size, Image.BICUBIC))\n",
    "\n",
    "    im = axes[1].imshow(attention_map_vis, cmap='hot', interpolation='bicubic')\n",
    "    axes[1].set_title('Attention Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "\n",
    "    # Overlay\n",
    "    axes[2].imshow(image, cmap='gray')\n",
    "    axes[2].imshow(attention_map_vis, cmap='hot', alpha=0.5, interpolation='bicubic')\n",
    "    axes[2].set_title('Attention Overlay')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_attention_patterns(model, processor, inputs, outputs, chest_xray):\n",
    "    \"\"\"Complete attention analysis pipeline\"\"\"\n",
    "\n",
    "    print(\"=== Analyzing Model Structure ===\")\n",
    "    vision_config, img_start, img_end = analyze_model_structure(model, processor, inputs)\n",
    "    print(f\"Image tokens: positions {img_start} to {img_end} ({img_end - img_start} tokens)\")\n",
    "\n",
    "    # Get attention data\n",
    "    attention_data = outputs.attentions\n",
    "    num_generated = len(attention_data)\n",
    "\n",
    "    print(f\"\\n=== Attention Analysis ===\")\n",
    "    print(f\"Generated {num_generated} tokens\")\n",
    "    print(f\"Each with {len(attention_data[0])} layers\")\n",
    "\n",
    "    # Analyze different layers\n",
    "    layers_to_analyze = [-1, -5, -10, 0]  # Last, mid-late, mid, first\n",
    "\n",
    "    for layer_idx in layers_to_analyze:\n",
    "        if abs(layer_idx) > len(attention_data[0]):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Layer {layer_idx} ---\")\n",
    "\n",
    "        # Extract attention for this layer\n",
    "        image_attn = extract_image_attention(attention_data, img_start, img_end, layer_idx)\n",
    "\n",
    "        # Average across heads\n",
    "        avg_attn = image_attn.mean(dim=0)\n",
    "\n",
    "        # Get attention from last generated token to image\n",
    "        last_gen_to_img = avg_attn[-1, :]\n",
    "\n",
    "        # Create spatial map\n",
    "        spatial_attn = create_spatial_attention_map(last_gen_to_img, vision_config)\n",
    "\n",
    "        # Visualize\n",
    "        fig = visualize_attention_on_image(chest_xray, spatial_attn.numpy(), vision_config)\n",
    "        plt.suptitle(f'Layer {layer_idx} Attention (Last Generated Token → Image)', fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "    # Aggregate attention across all generated tokens\n",
    "    print(\"\\n=== Aggregated Attention ===\")\n",
    "\n",
    "    all_attention_maps = []\n",
    "    for token_idx in range(num_generated):\n",
    "        token_attn = attention_data[token_idx][-1].cpu().float()  # Last layer\n",
    "        if len(token_attn.shape) == 4:\n",
    "            token_attn = token_attn[0]\n",
    "\n",
    "        # Average over heads\n",
    "        avg_token_attn = token_attn.mean(dim=0)\n",
    "\n",
    "        # Get position of this generated token\n",
    "        gen_pos = inputs['input_ids'].shape[1] + token_idx\n",
    "\n",
    "        if gen_pos < avg_token_attn.shape[0]:\n",
    "            # Attention from this generated token to image\n",
    "            gen_to_img = avg_token_attn[gen_pos, img_start:img_end]\n",
    "            spatial_map = create_spatial_attention_map(gen_to_img, vision_config)\n",
    "            all_attention_maps.append(spatial_map)\n",
    "\n",
    "    # Average all attention maps\n",
    "    if all_attention_maps:\n",
    "        avg_attention_map = torch.stack(all_attention_maps).mean(dim=0)\n",
    "\n",
    "        fig = visualize_attention_on_image(chest_xray, avg_attention_map.numpy(), vision_config)\n",
    "        plt.suptitle('Average Attention from All Generated Tokens', fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "    # Head-specific analysis\n",
    "    print(\"\\n=== Head-Specific Analysis ===\")\n",
    "    last_layer_attn = attention_data[0][-1].cpu().float()[0]  # Shape: [heads, seq, seq]\n",
    "    num_heads = last_layer_attn.shape[0]\n",
    "\n",
    "    # Visualize top attending heads\n",
    "    head_importances = []\n",
    "    for h in range(num_heads):\n",
    "        head_attn_to_img = last_layer_attn[h, -1, img_start:img_end]\n",
    "        importance = head_attn_to_img.max().item()\n",
    "        head_importances.append((h, importance))\n",
    "\n",
    "    # Sort by importance\n",
    "    head_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Visualize top 8 heads\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, (head_idx, importance) in enumerate(head_importances[:8]):\n",
    "        head_attn = last_layer_attn[head_idx, -1, img_start:img_end]\n",
    "        spatial_map = create_spatial_attention_map(head_attn, vision_config)\n",
    "\n",
    "        # Smooth interpolation\n",
    "        attn_tensor = torch.tensor(spatial_map).unsqueeze(0).unsqueeze(0)\n",
    "        upsampled = F.interpolate(attn_tensor, size=(224, 224), mode='bicubic', align_corners=False)\n",
    "\n",
    "        im = axes[idx].imshow(upsampled.squeeze().numpy(), cmap='hot', interpolation='bicubic')\n",
    "        axes[idx].set_title(f'Head {head_idx} (max: {importance:.3f})')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.suptitle('Top 8 Attention Heads (Last Layer)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'vision_config': vision_config,\n",
    "        'image_token_range': (img_start, img_end),\n",
    "        'avg_attention_map': avg_attention_map if 'avg_attention_map' in locals() else None,\n",
    "        'head_importances': head_importances\n",
    "    }\n",
    "\n",
    "# Modified main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have already loaded the model and generated outputs\n",
    "    # Run the improved analysis\n",
    "\n",
    "    results = analyze_attention_patterns(\n",
    "        model,\n",
    "        processor,\n",
    "        inputs_gpu,\n",
    "        outputs,\n",
    "        xray_pil_image\n",
    "    )\n",
    "\n",
    "    # Save enhanced results\n",
    "    torch.save(results, 'enhanced_chest_xray_attention.pt')\n",
    "    print(\"\\n✓ Enhanced attention analysis complete!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CQcompklXpEW",
    "outputId": "f6a305fe-7e03-4b1b-de9d-a2ca44631498"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "def enhance_attention_map(attention_map, enhance_contrast=True, threshold_percentile=80):\n",
    "    \"\"\"Enhance attention map for better visualization\"\"\"\n",
    "\n",
    "    # Convert to numpy if needed\n",
    "    if torch.is_tensor(attention_map):\n",
    "        attention_map = attention_map.numpy()\n",
    "\n",
    "    # Apply Gaussian smoothing for continuity\n",
    "    attention_map = gaussian_filter(attention_map, sigma=0.5)\n",
    "\n",
    "    if enhance_contrast:\n",
    "        # Enhance contrast using percentile-based normalization\n",
    "        threshold = np.percentile(attention_map, threshold_percentile)\n",
    "        attention_map = np.clip(attention_map - threshold, 0, None)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        if attention_map.max() > 0:\n",
    "            attention_map = attention_map / attention_map.max()\n",
    "\n",
    "    return attention_map\n",
    "\n",
    "def create_advanced_overlay(image, attention_map, title=\"\", cmap='jet', alpha=0.6):\n",
    "    \"\"\"Create advanced attention overlay with contours and annotations\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    # 1. Original image\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title('Original X-ray')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 2. Enhanced attention heatmap\n",
    "    enhanced_attn = enhance_attention_map(attention_map.copy())\n",
    "    im = axes[1].imshow(enhanced_attn, cmap=cmap, aspect='auto')\n",
    "    axes[1].set_title('Enhanced Attention')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "\n",
    "    # 3. Overlay with contours\n",
    "    axes[2].imshow(image, cmap='gray')\n",
    "\n",
    "    # Resize attention to match image\n",
    "    h, w = image.size[::-1]\n",
    "    attention_resized = cv2.resize(enhanced_attn, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Create contours for high attention regions\n",
    "    threshold = np.percentile(attention_resized, 90)\n",
    "    binary_mask = (attention_resized > threshold).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Overlay heatmap\n",
    "    axes[2].imshow(attention_resized, cmap=cmap, alpha=alpha)\n",
    "\n",
    "    # Draw contours\n",
    "    for contour in contours:\n",
    "        contour = contour.squeeze()\n",
    "        if len(contour) > 2:\n",
    "            axes[2].plot(contour[:, 0], contour[:, 1], 'w-', linewidth=2, alpha=0.8)\n",
    "\n",
    "    axes[2].set_title('Attention Regions')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    # 4. Focus regions with bounding boxes\n",
    "    axes[3].imshow(image, cmap='gray')\n",
    "\n",
    "    # Find regions of interest\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w * h > 100:  # Filter small regions\n",
    "            rect = Rectangle((x, y), w, h, linewidth=2,\n",
    "                           edgecolor='red', facecolor='none',\n",
    "                           linestyle='--', alpha=0.8)\n",
    "            axes[3].add_patch(rect)\n",
    "\n",
    "    axes[3].set_title('Regions of Interest')\n",
    "    axes[3].axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def attention_token_analysis(attention_data, generated_text, processor, image_start, image_end):\n",
    "    \"\"\"Analyze which tokens attend to which image regions\"\"\"\n",
    "\n",
    "    # Tokenize generated text to understand what each token represents\n",
    "    generated_tokens = processor.tokenizer.tokenize(generated_text)\n",
    "\n",
    "    # Create a mapping of token positions to words\n",
    "    token_word_map = []\n",
    "    current_word = \"\"\n",
    "    for token in generated_tokens:\n",
    "        if token.startswith(\"▁\"):  # New word in sentencepiece\n",
    "            if current_word:\n",
    "                token_word_map.append(current_word)\n",
    "            current_word = token[1:]\n",
    "        else:\n",
    "            current_word += token\n",
    "    if current_word:\n",
    "        token_word_map.append(current_word)\n",
    "\n",
    "    # Analyze attention for key medical terms\n",
    "    medical_keywords = ['lung', 'heart', 'chest', 'normal', 'clear', 'opacity',\n",
    "                       'consolidation', 'effusion', 'pneumonia', 'cardiomegaly']\n",
    "\n",
    "    keyword_attention = {}\n",
    "\n",
    "    for idx, word in enumerate(token_word_map):\n",
    "        word_lower = word.lower()\n",
    "        for keyword in medical_keywords:\n",
    "            if keyword in word_lower:\n",
    "                # Get attention for this token\n",
    "                if idx < len(attention_data):\n",
    "                    token_attn = attention_data[idx][-1].cpu().float()  # Last layer\n",
    "                    if len(token_attn.shape) == 4:\n",
    "                        token_attn = token_attn[0]\n",
    "\n",
    "                    # Average over heads\n",
    "                    avg_attn = token_attn.mean(dim=0)\n",
    "\n",
    "                    # Get attention to image\n",
    "                    gen_pos = image_end + idx\n",
    "                    if gen_pos < avg_attn.shape[0]:\n",
    "                        attn_to_img = avg_attn[gen_pos, image_start:image_end]\n",
    "                        keyword_attention[f\"{word} (token {idx})\"] = attn_to_img\n",
    "\n",
    "    return token_word_map, keyword_attention\n",
    "\n",
    "def visualize_keyword_attention(chest_xray, keyword_attention, vision_config):\n",
    "    \"\"\"Visualize attention for specific medical keywords\"\"\"\n",
    "\n",
    "    if not keyword_attention:\n",
    "        print(\"No medical keywords found in generated text\")\n",
    "        return\n",
    "\n",
    "    num_keywords = len(keyword_attention)\n",
    "    cols = min(4, num_keywords)\n",
    "    rows = (num_keywords + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    if num_keywords == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.ravel()\n",
    "\n",
    "    for idx, (keyword, attention) in enumerate(keyword_attention.items()):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "\n",
    "        # Create spatial map\n",
    "        spatial_attn = create_spatial_attention_map(attention, vision_config)\n",
    "        enhanced_attn = enhance_attention_map(spatial_attn.numpy())\n",
    "\n",
    "        # Resize to image size\n",
    "        h, w = chest_xray.size[::-1]\n",
    "        attention_resized = cv2.resize(enhanced_attn, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Plot\n",
    "        axes[idx].imshow(chest_xray, cmap='gray')\n",
    "        axes[idx].imshow(attention_resized, cmap='hot', alpha=0.6)\n",
    "        axes[idx].set_title(f'Attention for: {keyword}')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(keyword_attention), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.suptitle('Token-Specific Attention Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_attention_summary(model, processor, inputs, outputs, chest_xray, medical_report):\n",
    "    \"\"\"Create comprehensive attention analysis summary\"\"\"\n",
    "\n",
    "    print(\"=== Enhanced Attention Analysis ===\")\n",
    "\n",
    "    # Get model structure info\n",
    "    vision_config, img_start, img_end = analyze_model_structure(model, processor, inputs)\n",
    "\n",
    "    # 1. Layer-wise progression\n",
    "    print(\"\\n1. Layer-wise Attention Progression\")\n",
    "    layers = [0, 8, 16, 24, -1]  # Sample across network depth\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(layers), figsize=(20, 4))\n",
    "\n",
    "    for idx, layer in enumerate(layers):\n",
    "        image_attn = extract_image_attention(outputs.attentions, img_start, img_end, layer)\n",
    "        avg_attn = image_attn.mean(dim=0)\n",
    "        last_gen_to_img = avg_attn[-1, :]\n",
    "        spatial_attn = create_spatial_attention_map(last_gen_to_img, vision_config)\n",
    "        enhanced = enhance_attention_map(spatial_attn.numpy())\n",
    "\n",
    "        im = axes[idx].imshow(enhanced, cmap='hot', aspect='auto')\n",
    "        axes[idx].set_title(f'Layer {layer if layer >= 0 else 34 + layer}')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.suptitle('Attention Evolution Across Layers', fontsize=16)\n",
    "    plt.colorbar(im, ax=axes, fraction=0.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Aggregate attention with advanced overlay\n",
    "    print(\"\\n2. Creating Enhanced Attention Overlay\")\n",
    "    all_attention_maps = []\n",
    "\n",
    "    for token_idx in range(len(outputs.attentions)):\n",
    "        token_attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "        if len(token_attn.shape) == 4:\n",
    "            token_attn = token_attn[0]\n",
    "\n",
    "        avg_token_attn = token_attn.mean(dim=0)\n",
    "        gen_pos = inputs['input_ids'].shape[1] + token_idx\n",
    "\n",
    "        if gen_pos < avg_token_attn.shape[0]:\n",
    "            gen_to_img = avg_token_attn[gen_pos, img_start:img_end]\n",
    "            spatial_map = create_spatial_attention_map(gen_to_img, vision_config)\n",
    "            all_attention_maps.append(spatial_map)\n",
    "\n",
    "    if all_attention_maps:\n",
    "        avg_attention_map = torch.stack(all_attention_maps).mean(dim=0)\n",
    "        create_advanced_overlay(chest_xray, avg_attention_map,\n",
    "                              \"Enhanced Attention Analysis\", cmap='jet')\n",
    "        plt.show()\n",
    "\n",
    "    # 3. Token-specific attention\n",
    "    print(\"\\n3. Analyzing Token-Specific Attention\")\n",
    "    token_word_map, keyword_attention = attention_token_analysis(\n",
    "        outputs.attentions, medical_report, processor, img_start, img_end\n",
    "    )\n",
    "\n",
    "    if keyword_attention:\n",
    "        visualize_keyword_attention(chest_xray, keyword_attention, vision_config)\n",
    "        plt.show()\n",
    "\n",
    "    # 4. Attention statistics\n",
    "    print(\"\\n4. Attention Statistics\")\n",
    "\n",
    "    # Calculate attention entropy for each head\n",
    "    last_layer_attn = outputs.attentions[0][-1].cpu().float()[0]\n",
    "    head_entropies = []\n",
    "\n",
    "    for h in range(last_layer_attn.shape[0]):\n",
    "        head_attn = last_layer_attn[h, -1, img_start:img_end]\n",
    "        # Normalize to probability distribution\n",
    "        head_attn_norm = F.softmax(head_attn, dim=0)\n",
    "        # Calculate entropy\n",
    "        entropy = -(head_attn_norm * torch.log(head_attn_norm + 1e-8)).sum()\n",
    "        head_entropies.append(entropy.item())\n",
    "\n",
    "    # Plot entropy distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(head_entropies)), head_entropies)\n",
    "    plt.xlabel('Head Index')\n",
    "    plt.ylabel('Attention Entropy')\n",
    "    plt.title('Attention Entropy by Head (Lower = More Focused)')\n",
    "    plt.axhline(y=np.mean(head_entropies), color='r', linestyle='--',\n",
    "                label=f'Mean: {np.mean(head_entropies):.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Find most focused heads\n",
    "    focused_heads = np.argsort(head_entropies)[:5]\n",
    "    print(f\"\\nMost focused heads: {focused_heads.tolist()}\")\n",
    "    print(f\"Their entropies: {[head_entropies[h] for h in focused_heads]}\")\n",
    "\n",
    "    return {\n",
    "        'vision_config': vision_config,\n",
    "        'token_word_map': token_word_map,\n",
    "        'keyword_attention': keyword_attention,\n",
    "        'head_entropies': head_entropies,\n",
    "        'avg_attention_map': avg_attention_map if 'avg_attention_map' in locals() else None\n",
    "    }\n",
    "\n",
    "# Helper function from previous code\n",
    "def create_spatial_attention_map(attention_weights, vision_config):\n",
    "    \"\"\"Convert linear attention to spatial map\"\"\"\n",
    "    num_patches_per_side = int(np.sqrt(vision_config['vision_tokens']))\n",
    "\n",
    "    if len(attention_weights.shape) == 1:\n",
    "        spatial_map = attention_weights.reshape(num_patches_per_side, num_patches_per_side)\n",
    "    else:\n",
    "        spatial_map = attention_weights.mean(dim=0).reshape(num_patches_per_side, num_patches_per_side)\n",
    "\n",
    "    return spatial_map\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the enhanced analysis\n",
    "    results = create_attention_summary(\n",
    "        model,\n",
    "        processor,\n",
    "        inputs_gpu,\n",
    "        outputs,\n",
    "        xray_pil_image,\n",
    "        medical_report\n",
    "    )\n",
    "\n",
    "    print(\"\\n✓ Enhanced visualization complete!\")\n",
    "    print(f\"Analyzed {len(results['token_word_map'])} tokens\")\n",
    "    print(f\"Found {len(results.get('keyword_attention', {}))} medical keywords\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UYZiUvGnddSu",
    "outputId": "448f635f-5bfa-467b-a890-b98eb9c25805"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "def create_fixed_visualization(chest_xray, attention_weights, title=\"Medical Attention Analysis\"):\n",
    "    \"\"\"Fixed visualization with proper interpolation and overlay\"\"\"\n",
    "\n",
    "    # Ensure attention is properly shaped\n",
    "    n_tokens = len(attention_weights)\n",
    "    grid_size = int(np.sqrt(n_tokens))\n",
    "\n",
    "    if grid_size * grid_size != n_tokens:\n",
    "        print(f\"Warning: {n_tokens} tokens doesn't form perfect square, using {grid_size}x{grid_size}\")\n",
    "        # Truncate or pad\n",
    "        if grid_size * grid_size < n_tokens:\n",
    "            attention_weights = attention_weights[:grid_size * grid_size]\n",
    "        else:\n",
    "            padded = torch.zeros(grid_size * grid_size)\n",
    "            padded[:n_tokens] = attention_weights\n",
    "            attention_weights = padded\n",
    "\n",
    "    # Reshape to 2D\n",
    "    attention_2d = attention_weights.reshape(grid_size, grid_size)\n",
    "    if torch.is_tensor(attention_2d):\n",
    "        attention_2d = attention_2d.numpy()\n",
    "\n",
    "    # Apply smoothing\n",
    "    attention_smooth = gaussian_filter(attention_2d, sigma=0.8)\n",
    "\n",
    "    # Robust normalization\n",
    "    vmin, vmax = np.percentile(attention_smooth, [10, 90])\n",
    "    if vmax > vmin:\n",
    "        attention_norm = np.clip((attention_smooth - vmin) / (vmax - vmin), 0, 1)\n",
    "    else:\n",
    "        attention_norm = attention_smooth\n",
    "\n",
    "    # Create figure with better layout\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Define grid\n",
    "    gs = fig.add_gridspec(2, 3, height_ratios=[1, 1], width_ratios=[1, 1, 1])\n",
    "\n",
    "    # 1. Original X-ray\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(chest_xray, cmap='gray')\n",
    "    ax1.set_title('Original Chest X-ray', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # 2. Raw attention grid\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    im2 = ax2.imshow(attention_2d, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "    ax2.set_title(f'Raw Attention Grid ({grid_size}×{grid_size})', fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "\n",
    "    # 3. Smoothed attention\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    im3 = ax3.imshow(attention_norm, cmap='jet', interpolation='bicubic', aspect='auto')\n",
    "    ax3.set_title('Smoothed & Normalized', fontsize=14)\n",
    "    ax3.axis('off')\n",
    "    plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "\n",
    "    # 4. High-resolution interpolation using cv2\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    # Resize attention to match image size\n",
    "    img_h, img_w = chest_xray.size[::-1]  # PIL uses (width, height)\n",
    "\n",
    "    # Use cv2 for better interpolation\n",
    "    attention_resized = cv2.resize(attention_norm, (img_w, img_h),\n",
    "                                   interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    im4 = ax4.imshow(attention_resized, cmap='hot', interpolation='bicubic')\n",
    "    ax4.set_title('High-res Attention Map', fontsize=14)\n",
    "    ax4.axis('off')\n",
    "    plt.colorbar(im4, ax=ax4, fraction=0.046)\n",
    "\n",
    "    # 5. Overlay on X-ray\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    ax5.imshow(chest_xray, cmap='gray')\n",
    "    ax5.imshow(attention_resized, cmap='hot', alpha=0.5, interpolation='bicubic')\n",
    "    ax5.set_title('Attention Overlay', fontsize=14)\n",
    "    ax5.axis('off')\n",
    "\n",
    "    # 6. Contour visualization\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax6.imshow(chest_xray, cmap='gray')\n",
    "\n",
    "    # Create contours at different levels\n",
    "    levels = [0.3, 0.5, 0.7, 0.9]\n",
    "    contours = ax6.contour(attention_resized, levels=levels,\n",
    "                          colors=['blue', 'green', 'yellow', 'red'],\n",
    "                          linewidths=2, alpha=0.8)\n",
    "    ax6.clabel(contours, inline=True, fontsize=10)\n",
    "    ax6.set_title('Attention Contours', fontsize=14)\n",
    "    ax6.axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, attention_resized\n",
    "\n",
    "def analyze_attention_regions(attention_map, chest_xray, threshold=0.7):\n",
    "    \"\"\"Analyze and annotate high-attention regions\"\"\"\n",
    "\n",
    "    # Create binary mask for high attention\n",
    "    high_attention = attention_map > threshold\n",
    "\n",
    "    # Find connected components\n",
    "    num_labels, labels = cv2.connectedComponents(high_attention.astype(np.uint8))\n",
    "\n",
    "    # Analyze each region\n",
    "    regions = []\n",
    "    for label in range(1, num_labels):\n",
    "        mask = labels == label\n",
    "\n",
    "        # Get region properties\n",
    "        coords = np.column_stack(np.where(mask))\n",
    "        if len(coords) > 0:\n",
    "            # Calculate center\n",
    "            center_y, center_x = coords.mean(axis=0)\n",
    "\n",
    "            # Calculate area\n",
    "            area = len(coords)\n",
    "\n",
    "            # Get bounding box\n",
    "            y_min, x_min = coords.min(axis=0)\n",
    "            y_max, x_max = coords.max(axis=0)\n",
    "\n",
    "            # Average attention in this region\n",
    "            avg_attention = attention_map[mask].mean()\n",
    "\n",
    "            regions.append({\n",
    "                'center': (center_x, center_y),\n",
    "                'area': area,\n",
    "                'bbox': (x_min, y_min, x_max - x_min, y_max - y_min),\n",
    "                'avg_attention': avg_attention,\n",
    "                'location': classify_location(center_x, center_y, chest_xray.size)\n",
    "            })\n",
    "\n",
    "    return regions\n",
    "\n",
    "def classify_location(x, y, img_size):\n",
    "    \"\"\"Classify anatomical location based on position\"\"\"\n",
    "    width, height = img_size\n",
    "\n",
    "    # Normalize coordinates\n",
    "    x_norm = x / width\n",
    "    y_norm = y / height\n",
    "\n",
    "    # Simple anatomical classification\n",
    "    if y_norm < 0.3:\n",
    "        vertical = \"upper\"\n",
    "    elif y_norm < 0.6:\n",
    "        vertical = \"middle\"\n",
    "    else:\n",
    "        vertical = \"lower\"\n",
    "\n",
    "    if x_norm < 0.35:\n",
    "        horizontal = \"right\"  # Note: chest X-rays are mirrored\n",
    "    elif x_norm < 0.65:\n",
    "        horizontal = \"central\"\n",
    "    else:\n",
    "        horizontal = \"left\"\n",
    "\n",
    "    return f\"{vertical} {horizontal}\"\n",
    "\n",
    "def create_clinical_summary(chest_xray, attention_map, regions, medical_report):\n",
    "    \"\"\"Create a clinical summary visualization\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Left: Annotated X-ray\n",
    "    axes[0].imshow(chest_xray, cmap='gray')\n",
    "    axes[0].imshow(attention_map, cmap='hot', alpha=0.4)\n",
    "\n",
    "    # Annotate regions\n",
    "    for i, region in enumerate(regions[:5]):  # Top 5 regions\n",
    "        x, y = region['center']\n",
    "        axes[0].scatter(x, y, c='yellow', s=100, marker='x', linewidths=3)\n",
    "        axes[0].annotate(f\"{i+1}\", (x, y), xytext=(x+10, y+10),\n",
    "                        color='yellow', fontsize=12, fontweight='bold',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "\n",
    "    axes[0].set_title('Attention Focus Areas', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Right: Clinical summary\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Analysis Summary', fontsize=14)\n",
    "\n",
    "    # Create text summary\n",
    "    summary_text = f\"Generated Report:\\n{medical_report}\\n\\n\"\n",
    "    summary_text += \"Attention Analysis:\\n\"\n",
    "    summary_text += f\"• Found {len(regions)} high-attention regions\\n\\n\"\n",
    "\n",
    "    summary_text += \"Top Focus Areas:\\n\"\n",
    "    for i, region in enumerate(regions[:5]):\n",
    "        summary_text += f\"{i+1}. {region['location'].title()}: \"\n",
    "        summary_text += f\"{region['avg_attention']:.1%} attention\\n\"\n",
    "\n",
    "    # Add text with better formatting\n",
    "    axes[1].text(0.05, 0.95, summary_text, transform=axes[1].transAxes,\n",
    "                fontsize=12, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Improved main analysis function\n",
    "def run_complete_analysis(model, processor, inputs, outputs, chest_xray, medical_report):\n",
    "    \"\"\"Complete analysis pipeline with fixes\"\"\"\n",
    "\n",
    "    print(\"=== RUNNING COMPLETE ANALYSIS ===\")\n",
    "\n",
    "    # Extract attention (using previous logic)\n",
    "    image_start, image_end = 1, 257  # Based on your findings\n",
    "\n",
    "    # Get attention from last layer, averaged across heads\n",
    "    last_attn = outputs.attentions[0][-1].cpu().float()\n",
    "    if len(last_attn.shape) == 4:\n",
    "        last_attn = last_attn[0]\n",
    "\n",
    "    # Average across heads and get attention to image\n",
    "    avg_attn = last_attn.mean(dim=0)\n",
    "    gen_pos = inputs['input_ids'].shape[1]  # First generated position\n",
    "\n",
    "    if gen_pos < avg_attn.shape[0]:\n",
    "        attention_to_image = avg_attn[gen_pos, image_start:image_end]\n",
    "    else:\n",
    "        # Fallback\n",
    "        attention_to_image = avg_attn[-1, image_start:image_end]\n",
    "\n",
    "    # Create visualizations\n",
    "    print(\"\\n1. Creating fixed visualization...\")\n",
    "    fig1, attention_resized = create_fixed_visualization(chest_xray, attention_to_image)\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze regions\n",
    "    print(\"\\n2. Analyzing attention regions...\")\n",
    "    regions = analyze_attention_regions(attention_resized, chest_xray, threshold=0.5)\n",
    "    regions.sort(key=lambda x: x['avg_attention'], reverse=True)\n",
    "\n",
    "    print(f\"Found {len(regions)} high-attention regions:\")\n",
    "    for i, region in enumerate(regions[:5]):\n",
    "        print(f\"  {i+1}. {region['location']}: {region['avg_attention']:.1%} attention\")\n",
    "\n",
    "    # Create clinical summary\n",
    "    print(\"\\n3. Creating clinical summary...\")\n",
    "    fig2 = create_clinical_summary(chest_xray, attention_resized, regions, medical_report)\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'attention_map': attention_resized,\n",
    "        'regions': regions,\n",
    "        'figures': [fig1, fig2]\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_complete_analysis(\n",
    "        model, processor, inputs_gpu, outputs, xray_pil_image, medical_report\n",
    "    )\n",
    "\n",
    "    print(\"\\n✓ Analysis complete!\")\n",
    "    print(f\"Identified {len(results['regions'])} regions of interest\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qf_BhwV6eWfP",
    "outputId": "ad15acc1-fa86-40fb-9183-5793ae2486e1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def debug_attention_structure(outputs, inputs):\n",
    "    \"\"\"Debug the attention structure to understand the issue\"\"\"\n",
    "\n",
    "    print(\"\\n=== DEBUGGING ATTENTION STRUCTURE ===\")\n",
    "\n",
    "    # Check outputs structure\n",
    "    print(f\"Number of generated tokens: {len(outputs.attentions)}\")\n",
    "    print(f\"Type of outputs.attentions: {type(outputs.attentions)}\")\n",
    "\n",
    "    if len(outputs.attentions) > 0:\n",
    "        print(f\"\\nFirst token attention structure:\")\n",
    "        first_attn = outputs.attentions[0]\n",
    "        print(f\"  Number of layers: {len(first_attn)}\")\n",
    "        print(f\"  Type: {type(first_attn)}\")\n",
    "\n",
    "        if len(first_attn) > 0:\n",
    "            last_layer = first_attn[-1]\n",
    "            print(f\"\\n  Last layer shape: {last_layer.shape}\")\n",
    "            print(f\"  Last layer dtype: {last_layer.dtype}\")\n",
    "            print(f\"  Last layer device: {last_layer.device}\")\n",
    "\n",
    "    # Check input structure\n",
    "    print(f\"\\nInput sequence length: {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def fixed_attention_extraction(outputs, inputs, image_start=1, image_end=257):\n",
    "    \"\"\"Fixed extraction that handles common issues\"\"\"\n",
    "\n",
    "    results = {\n",
    "        'success': False,\n",
    "        'attention_maps': [],\n",
    "        'aggregate': None,\n",
    "        'debug_info': {}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        num_tokens = len(outputs.attentions)\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        print(f\"\\nExtracting attention from {num_tokens} generated tokens\")\n",
    "        print(f\"Input length: {input_len}, Image range: {image_start}-{image_end}\")\n",
    "\n",
    "        # Collect attention maps for each generated token\n",
    "        valid_maps = []\n",
    "\n",
    "        for token_idx in range(min(num_tokens, 10)):  # Process first 10 tokens\n",
    "            try:\n",
    "                # Get attention for this token\n",
    "                token_attentions = outputs.attentions[token_idx]\n",
    "\n",
    "                # Use last layer\n",
    "                if isinstance(token_attentions, (list, tuple)) and len(token_attentions) > 0:\n",
    "                    last_layer_attn = token_attentions[-1]\n",
    "                else:\n",
    "                    print(f\"Skipping token {token_idx}: unexpected structure\")\n",
    "                    continue\n",
    "\n",
    "                # Move to CPU and convert to float\n",
    "                last_layer_attn = last_layer_attn.cpu().float()\n",
    "\n",
    "                # Remove batch dimension if present\n",
    "                if len(last_layer_attn.shape) == 4:\n",
    "                    last_layer_attn = last_layer_attn[0]\n",
    "\n",
    "                # Check shape\n",
    "                if len(last_layer_attn.shape) != 3:\n",
    "                    print(f\"Skipping token {token_idx}: unexpected shape {last_layer_attn.shape}\")\n",
    "                    continue\n",
    "\n",
    "                # Average over heads\n",
    "                avg_attn = last_layer_attn.mean(dim=0)  # Shape: [seq_len, seq_len]\n",
    "\n",
    "                # The position of the current generated token\n",
    "                gen_position = input_len + token_idx\n",
    "\n",
    "                # Check if position is valid\n",
    "                if gen_position >= avg_attn.shape[0]:\n",
    "                    # Use last available position\n",
    "                    gen_position = avg_attn.shape[0] - 1\n",
    "                    print(f\"Token {token_idx}: Using last position {gen_position}\")\n",
    "\n",
    "                # Extract attention from generated token to image tokens\n",
    "                if gen_position < avg_attn.shape[0] and image_end <= avg_attn.shape[1]:\n",
    "                    attn_to_image = avg_attn[gen_position, image_start:image_end]\n",
    "\n",
    "                    # Verify we got 256 values (16x16)\n",
    "                    if len(attn_to_image) == 256:\n",
    "                        attn_2d = attn_to_image.reshape(16, 16).numpy()\n",
    "                        valid_maps.append(attn_2d)\n",
    "\n",
    "                        if token_idx == 0:\n",
    "                            print(f\"First token attention stats: min={attn_2d.min():.4f}, max={attn_2d.max():.4f}, mean={attn_2d.mean():.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Token {token_idx}: Wrong number of image tokens: {len(attn_to_image)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing token {token_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"\\nSuccessfully extracted {len(valid_maps)} attention maps\")\n",
    "\n",
    "        if valid_maps:\n",
    "            results['success'] = True\n",
    "            results['attention_maps'] = valid_maps\n",
    "\n",
    "            # Create aggregate\n",
    "            aggregate = np.mean(valid_maps, axis=0)\n",
    "            results['aggregate'] = aggregate\n",
    "\n",
    "            results['debug_info'] = {\n",
    "                'num_valid_maps': len(valid_maps),\n",
    "                'aggregate_shape': aggregate.shape,\n",
    "                'aggregate_stats': {\n",
    "                    'min': float(aggregate.min()),\n",
    "                    'max': float(aggregate.max()),\n",
    "                    'mean': float(aggregate.mean()),\n",
    "                    'std': float(aggregate.std())\n",
    "                }\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during extraction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_working_visualization(chest_xray, results):\n",
    "    \"\"\"Create visualization that works with the extracted data\"\"\"\n",
    "\n",
    "    if not results['success']:\n",
    "        print(\"No valid attention data to visualize\")\n",
    "        return None\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "    # Create grid\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # 1. Original X-ray\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(chest_xray, cmap='gray')\n",
    "    ax1.set_title('Input X-ray', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # 2. First token attention\n",
    "    if results['attention_maps']:\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        first_attn = results['attention_maps'][0]\n",
    "        im2 = ax2.imshow(first_attn, cmap='hot', interpolation='bicubic')\n",
    "        ax2.set_title('First Token Attention', fontsize=14)\n",
    "        ax2.axis('off')\n",
    "        plt.colorbar(im2, ax=ax2, fraction=0.046)\n",
    "\n",
    "    # 3. Aggregate attention\n",
    "    if results['aggregate'] is not None:\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "        # Enhance contrast\n",
    "        agg = results['aggregate']\n",
    "        vmin, vmax = np.percentile(agg, [10, 90])\n",
    "        if vmax > vmin:\n",
    "            agg_norm = np.clip((agg - vmin) / (vmax - vmin), 0, 1)\n",
    "        else:\n",
    "            agg_norm = agg\n",
    "\n",
    "        im3 = ax3.imshow(agg_norm, cmap='jet', interpolation='bicubic')\n",
    "        ax3.set_title('Aggregate Attention (Enhanced)', fontsize=14)\n",
    "        ax3.axis('off')\n",
    "        plt.colorbar(im3, ax=ax3, fraction=0.046)\n",
    "\n",
    "    # 4. Overlay on X-ray\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.imshow(chest_xray, cmap='gray')\n",
    "\n",
    "    if results['aggregate'] is not None:\n",
    "        # Resize aggregate to match image\n",
    "        h, w = chest_xray.size[::-1]\n",
    "        agg_resized = cv2.resize(agg_norm, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "        ax4.imshow(agg_resized, cmap='hot', alpha=0.5)\n",
    "\n",
    "    ax4.set_title('Attention Overlay', fontsize=14)\n",
    "    ax4.axis('off')\n",
    "\n",
    "    # 5. Attention evolution\n",
    "    if len(results['attention_maps']) > 1:\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "        # Show how max attention changes over tokens\n",
    "        max_values = [m.max() for m in results['attention_maps']]\n",
    "        mean_values = [m.mean() for m in results['attention_maps']]\n",
    "\n",
    "        tokens = range(len(max_values))\n",
    "        ax5.plot(tokens, max_values, 'r-', label='Max attention', marker='o')\n",
    "        ax5.plot(tokens, mean_values, 'b-', label='Mean attention', marker='s')\n",
    "        ax5.set_xlabel('Token Index')\n",
    "        ax5.set_ylabel('Attention Value')\n",
    "        ax5.set_title('Attention Evolution')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Statistics\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax6.axis('off')\n",
    "\n",
    "    stats_text = \"Attention Statistics\\n\" + \"=\"*25 + \"\\n\\n\"\n",
    "    stats_text += f\"Maps extracted: {results['debug_info']['num_valid_maps']}\\n\\n\"\n",
    "\n",
    "    if 'aggregate_stats' in results['debug_info']:\n",
    "        stats = results['debug_info']['aggregate_stats']\n",
    "        stats_text += \"Aggregate attention:\\n\"\n",
    "        stats_text += f\"  Min:  {stats['min']:.4f}\\n\"\n",
    "        stats_text += f\"  Max:  {stats['max']:.4f}\\n\"\n",
    "        stats_text += f\"  Mean: {stats['mean']:.4f}\\n\"\n",
    "        stats_text += f\"  Std:  {stats['std']:.4f}\\n\\n\"\n",
    "\n",
    "    # Find high attention regions\n",
    "    if results['aggregate'] is not None:\n",
    "        threshold = np.percentile(agg_norm, 80)\n",
    "        high_attn = agg_norm > threshold\n",
    "        y_coords, x_coords = np.where(high_attn)\n",
    "\n",
    "        if len(y_coords) > 0:\n",
    "            center_y = y_coords.mean() / 16\n",
    "            center_x = x_coords.mean() / 16\n",
    "\n",
    "            stats_text += \"Primary focus region:\\n\"\n",
    "            if center_y < 0.4:\n",
    "                stats_text += \"  • Upper \"\n",
    "            elif center_y > 0.6:\n",
    "                stats_text += \"  • Lower \"\n",
    "            else:\n",
    "                stats_text += \"  • Middle \"\n",
    "\n",
    "            if center_x < 0.4:\n",
    "                stats_text += \"left\\n\"\n",
    "            elif center_x > 0.6:\n",
    "                stats_text += \"right\\n\"\n",
    "            else:\n",
    "                stats_text += \"center\\n\"\n",
    "\n",
    "    ax6.text(0.05, 0.95, stats_text, transform=ax6.transAxes,\n",
    "             fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "    plt.suptitle('MedGemma Attention Analysis (Fixed)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Run the debugging and fixed visualization\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== RUNNING FIXED ANALYSIS ===\")\n",
    "\n",
    "    # First debug the structure\n",
    "    debug_attention_structure(outputs, inputs_gpu)\n",
    "\n",
    "    # Extract attention with better error handling\n",
    "    results = fixed_attention_extraction(outputs, inputs_gpu)\n",
    "\n",
    "    # Create visualization if extraction succeeded\n",
    "    if results['success']:\n",
    "        fig = create_working_visualization(xray_pil_image, results)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\n✓ Successfully visualized {results['debug_info']['num_valid_maps']} attention maps\")\n",
    "    else:\n",
    "        print(\"\\n✗ Failed to extract valid attention maps\")\n",
    "\n",
    "    # Print debug info\n",
    "    print(\"\\nDebug Information:\")\n",
    "    for key, value in results['debug_info'].items():\n",
    "        print(f\"  {key}: {value}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Mqmj0z_-fzYG",
    "outputId": "b4d4798e-98b9-4739-ecce-5f155a968e4c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def analyze_medgemma_attention(outputs, inputs, chest_xray, medical_report):\n",
    "    \"\"\"Properly handle MedGemma's attention mechanism\"\"\"\n",
    "\n",
    "    print(\"\\n=== MEDGEMMA ATTENTION ANALYSIS ===\")\n",
    "\n",
    "    # Configuration based on debugging\n",
    "    image_start = 1\n",
    "    image_end = 257\n",
    "    input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "    print(f\"Input length: {input_len}\")\n",
    "    print(f\"Image tokens: {image_start} to {image_end}\")\n",
    "\n",
    "    # For MedGemma, it seems the attention matrix doesn't grow\n",
    "    # Instead, we need to look at how the existing positions attend to the image\n",
    "\n",
    "    results = extract_proper_attention(outputs, input_len, image_start, image_end)\n",
    "\n",
    "    if results['success']:\n",
    "        create_comprehensive_viz(chest_xray, results, medical_report)\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_proper_attention(outputs, input_len, image_start, image_end):\n",
    "    \"\"\"Extract attention properly for MedGemma's architecture\"\"\"\n",
    "\n",
    "    results = {\n",
    "        'success': False,\n",
    "        'method': 'fixed_matrix',\n",
    "        'attention_maps': [],\n",
    "        'token_positions': [],\n",
    "        'aggregate': None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        num_generated = len(outputs.attentions)\n",
    "        print(f\"\\nProcessing {num_generated} generated tokens\")\n",
    "\n",
    "        # For each generated token\n",
    "        for token_idx in range(min(num_generated, 20)):\n",
    "            token_attn = outputs.attentions[token_idx][-1].cpu().float()  # Last layer\n",
    "\n",
    "            if len(token_attn.shape) == 4:\n",
    "                token_attn = token_attn[0]  # Remove batch\n",
    "\n",
    "            # Average over heads\n",
    "            avg_attn = token_attn.mean(dim=0)  # Shape: [seq_len, seq_len]\n",
    "\n",
    "            # For MedGemma with fixed attention size, we need a different approach\n",
    "            # Option 1: Look at the last valid position's attention to image\n",
    "            last_pos = avg_attn.shape[0] - 1\n",
    "\n",
    "            # Option 2: Look at attention from the \"current\" position\n",
    "            # This might be the last position or a special position\n",
    "\n",
    "            # Try to find which position corresponds to the current generated token\n",
    "            # by looking for the position with highest attention entropy (most active)\n",
    "\n",
    "            # Calculate entropy for each position\n",
    "            entropies = []\n",
    "            for pos in range(avg_attn.shape[0]):\n",
    "                attn_dist = avg_attn[pos, :]\n",
    "                # Normalize to probability\n",
    "                attn_prob = torch.softmax(attn_dist, dim=0)\n",
    "                entropy = -(attn_prob * torch.log(attn_prob + 1e-10)).sum()\n",
    "                entropies.append(entropy.item())\n",
    "\n",
    "            # The position with highest entropy might be the \"active\" position\n",
    "            active_pos = np.argmax(entropies)\n",
    "\n",
    "            # Also try the last position\n",
    "            positions_to_try = [last_pos, active_pos, input_len - 1]\n",
    "\n",
    "            extracted = False\n",
    "            for pos in positions_to_try:\n",
    "                if 0 <= pos < avg_attn.shape[0] and image_end <= avg_attn.shape[1]:\n",
    "                    attn_to_image = avg_attn[pos, image_start:image_end]\n",
    "\n",
    "                    if len(attn_to_image) == 256:\n",
    "                        attn_2d = attn_to_image.reshape(16, 16).numpy()\n",
    "                        results['attention_maps'].append(attn_2d)\n",
    "                        results['token_positions'].append((token_idx, pos))\n",
    "\n",
    "                        if token_idx == 0:\n",
    "                            print(f\"Token 0: Using position {pos} (entropy: {entropies[pos]:.2f})\")\n",
    "                            print(f\"  Attention stats: min={attn_2d.min():.4f}, max={attn_2d.max():.4f}\")\n",
    "\n",
    "                        extracted = True\n",
    "                        break\n",
    "\n",
    "            if not extracted:\n",
    "                print(f\"Failed to extract attention for token {token_idx}\")\n",
    "\n",
    "        if results['attention_maps']:\n",
    "            results['success'] = True\n",
    "            results['aggregate'] = np.mean(results['attention_maps'], axis=0)\n",
    "            print(f\"\\nSuccessfully extracted {len(results['attention_maps'])} attention maps\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_comprehensive_viz(chest_xray, results, medical_report):\n",
    "    \"\"\"Create comprehensive visualization for MedGemma attention\"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # 1. Input X-ray with annotation\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(chest_xray, cmap='gray')\n",
    "    ax1.set_title('Input Chest X-ray', fontsize=12)\n",
    "\n",
    "    # Add grid to show patch regions\n",
    "    h, w = chest_xray.size[::-1]\n",
    "    patch_h, patch_w = h // 16, w // 16\n",
    "    for i in range(17):\n",
    "        ax1.axhline(i * patch_h, color='blue', alpha=0.2, linewidth=0.5)\n",
    "        ax1.axvline(i * patch_w, color='blue', alpha=0.2, linewidth=0.5)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # 2-5. Sample attention maps from different tokens\n",
    "    sample_indices = [0, len(results['attention_maps'])//3,\n",
    "                     2*len(results['attention_maps'])//3, -1]\n",
    "\n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        if 0 <= sample_idx < len(results['attention_maps']) or sample_idx == -1:\n",
    "            ax = fig.add_subplot(gs[0, idx+1] if idx < 3 else gs[1, 0])\n",
    "\n",
    "            attn_map = results['attention_maps'][sample_idx]\n",
    "            token_idx, pos = results['token_positions'][sample_idx]\n",
    "\n",
    "            # Enhance contrast\n",
    "            vmin, vmax = np.percentile(attn_map, [20, 80])\n",
    "            attn_enhanced = np.clip((attn_map - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "            im = ax.imshow(attn_enhanced, cmap='hot', interpolation='bicubic')\n",
    "            ax.set_title(f'Token {token_idx} (pos {pos})', fontsize=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add grid\n",
    "            for i in range(17):\n",
    "                ax.axhline(i - 0.5, color='white', alpha=0.2, linewidth=0.5)\n",
    "                ax.axvline(i - 0.5, color='white', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "    # 6. Aggregate attention\n",
    "    ax6 = fig.add_subplot(gs[1, 1])\n",
    "    agg = results['aggregate']\n",
    "    vmin, vmax = np.percentile(agg, [10, 90])\n",
    "    agg_enhanced = np.clip((agg - vmin) / (vmax - vmin + 1e-8), 0, 1)\n",
    "\n",
    "    im = ax6.imshow(agg_enhanced, cmap='jet', interpolation='bicubic')\n",
    "    ax6.set_title('Average Attention Across All Tokens', fontsize=12)\n",
    "    ax6.axis('off')\n",
    "    plt.colorbar(im, ax=ax6, fraction=0.046)\n",
    "\n",
    "    # 7. Overlay on X-ray\n",
    "    ax7 = fig.add_subplot(gs[1, 2])\n",
    "    ax7.imshow(chest_xray, cmap='gray')\n",
    "\n",
    "    # Resize attention\n",
    "    agg_resized = cv2.resize(agg_enhanced, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "    ax7.imshow(agg_resized, cmap='hot', alpha=0.5)\n",
    "    ax7.set_title('Attention Overlay', fontsize=12)\n",
    "    ax7.axis('off')\n",
    "\n",
    "    # 8. Attention distribution\n",
    "    ax8 = fig.add_subplot(gs[1, 3])\n",
    "    ax8.hist(agg.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax8.axvline(agg.mean(), color='red', linestyle='--', label=f'Mean: {agg.mean():.4f}')\n",
    "    ax8.axvline(np.percentile(agg, 90), color='green', linestyle='--',\n",
    "                label=f'90th percentile: {np.percentile(agg, 90):.4f}')\n",
    "    ax8.set_xlabel('Attention Value')\n",
    "    ax8.set_ylabel('Frequency')\n",
    "    ax8.set_title('Attention Distribution')\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "\n",
    "    # 9-11. Regional analysis\n",
    "    # Divide image into regions\n",
    "    regions = {\n",
    "        'Upper Left': (0, 5, 0, 5),\n",
    "        'Upper Center': (0, 5, 5, 11),\n",
    "        'Upper Right': (0, 5, 11, 16),\n",
    "        'Middle Left': (5, 11, 0, 5),\n",
    "        'Middle Center': (5, 11, 5, 11),\n",
    "        'Middle Right': (5, 11, 11, 16),\n",
    "        'Lower Left': (11, 16, 0, 5),\n",
    "        'Lower Center': (11, 16, 5, 11),\n",
    "        'Lower Right': (11, 16, 11, 16)\n",
    "    }\n",
    "\n",
    "    # Calculate average attention per region\n",
    "    region_attention = {}\n",
    "    for region_name, (r1, r2, c1, c2) in regions.items():\n",
    "        region_avg = agg[r1:r2, c1:c2].mean()\n",
    "        region_attention[region_name] = region_avg\n",
    "\n",
    "    # Sort regions by attention\n",
    "    sorted_regions = sorted(region_attention.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Plot top regions\n",
    "    ax9 = fig.add_subplot(gs[2, 0])\n",
    "    region_names = [r[0] for r in sorted_regions[:5]]\n",
    "    region_values = [r[1] for r in sorted_regions[:5]]\n",
    "\n",
    "    bars = ax9.barh(region_names, region_values, color='skyblue', edgecolor='navy')\n",
    "    ax9.set_xlabel('Average Attention')\n",
    "    ax9.set_title('Top 5 Attention Regions')\n",
    "    ax9.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # Add values on bars\n",
    "    for bar, val in zip(bars, region_values):\n",
    "        ax9.text(bar.get_width() + 0.0001, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "    # 10. Attention focus visualization\n",
    "    ax10 = fig.add_subplot(gs[2, 1])\n",
    "    ax10.imshow(agg_enhanced, cmap='gray')\n",
    "\n",
    "    # Highlight top 3 regions\n",
    "    colors = ['red', 'yellow', 'green']\n",
    "    for idx, (region_name, _) in enumerate(sorted_regions[:3]):\n",
    "        if idx < 3:\n",
    "            r1, r2, c1, c2 = regions[region_name]\n",
    "            rect = Rectangle((c1-0.5, r1-0.5), c2-c1, r2-r1,\n",
    "                           linewidth=2, edgecolor=colors[idx],\n",
    "                           facecolor='none', linestyle='--')\n",
    "            ax10.add_patch(rect)\n",
    "            ax10.text(c1, r1-1, f'#{idx+1}', color=colors[idx],\n",
    "                     fontweight='bold', fontsize=10)\n",
    "\n",
    "    ax10.set_title('Top 3 Focus Regions', fontsize=12)\n",
    "    ax10.axis('off')\n",
    "\n",
    "    # 11. Clinical correlation\n",
    "    ax11 = fig.add_subplot(gs[2, 2:])\n",
    "    ax11.axis('off')\n",
    "\n",
    "    # Create clinical summary\n",
    "    summary = f\"CLINICAL CORRELATION\\n{'='*50}\\n\\n\"\n",
    "    summary += f\"Generated Report:\\n{medical_report[:150]}...\\n\\n\"\n",
    "    summary += f\"Attention Analysis:\\n\"\n",
    "    summary += f\"• Primary focus: {sorted_regions[0][0]} ({sorted_regions[0][1]:.4f})\\n\"\n",
    "    summary += f\"• Secondary focus: {sorted_regions[1][0]} ({sorted_regions[1][1]:.4f})\\n\"\n",
    "    summary += f\"• Tertiary focus: {sorted_regions[2][0]} ({sorted_regions[2][1]:.4f})\\n\\n\"\n",
    "\n",
    "    # Interpret based on report content\n",
    "    if \"clear\" in medical_report.lower() and \"bilateral\" in medical_report.lower():\n",
    "        summary += \"Interpretation: Model performed systematic bilateral assessment\\n\"\n",
    "        summary += \"consistent with report of clear lung fields.\"\n",
    "\n",
    "    ax11.text(0.05, 0.95, summary, transform=ax11.transAxes,\n",
    "             fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "    plt.suptitle('MedGemma Comprehensive Attention Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_medgemma_attention(outputs, inputs_gpu, xray_pil_image, medical_report)\n",
    "\n",
    "    print(\"\\n✓ Analysis complete!\")\n",
    "    if results['success']:\n",
    "        print(f\"Method used: {results['method']}\")\n",
    "        print(f\"Extracted {len(results['attention_maps'])} attention maps\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JqRjpbCeiTLT",
    "outputId": "088b9620-4ac8-42db-c192-8438333f64f4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MedGemmaRelevancyAnalyzer:\n",
    "    \"\"\"\n",
    "    Fixed implementation for MedGemma's specific architecture\n",
    "    Handles bfloat16 and fixed attention matrix sizes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval()\n",
    "\n",
    "    def compute_simple_relevancy(self, outputs, inputs, token_idx):\n",
    "        \"\"\"\n",
    "        Simplified relevancy using layer-weighted attention\n",
    "        Works with MedGemma's architecture\n",
    "        \"\"\"\n",
    "\n",
    "        image_start, image_end = 1, 257\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        relevancy_scores = []\n",
    "\n",
    "        # Process attention from each layer\n",
    "        if token_idx < len(outputs.attentions):\n",
    "            token_attentions = outputs.attentions[token_idx]\n",
    "\n",
    "            for layer_idx, layer_attn in enumerate(token_attentions):\n",
    "                if torch.is_tensor(layer_attn):\n",
    "                    # Convert to float32 for computation\n",
    "                    layer_attn = layer_attn.cpu().float()\n",
    "\n",
    "                    if len(layer_attn.shape) == 4:\n",
    "                        layer_attn = layer_attn[0]  # Remove batch\n",
    "\n",
    "                    # Average over heads\n",
    "                    layer_attn = layer_attn.mean(dim=0)\n",
    "\n",
    "                    # MedGemma uses fixed attention size\n",
    "                    # Find the right position to extract from\n",
    "                    if layer_attn.shape[0] == layer_attn.shape[1]:\n",
    "                        # Square attention matrix\n",
    "                        if layer_attn.shape[0] > input_length:\n",
    "                            # Use last position that makes sense\n",
    "                            src_pos = min(input_length + token_idx, layer_attn.shape[0] - 1)\n",
    "                        else:\n",
    "                            src_pos = layer_attn.shape[0] - 1\n",
    "                    else:\n",
    "                        src_pos = -1\n",
    "\n",
    "                    # Extract attention to image tokens\n",
    "                    if src_pos >= 0 and image_end <= layer_attn.shape[1]:\n",
    "                        attn_to_image = layer_attn[src_pos, image_start:image_end]\n",
    "\n",
    "                        # Weight by layer depth (later layers more important)\n",
    "                        layer_weight = (layer_idx + 1) / len(token_attentions)\n",
    "                        weighted_attn = attn_to_image * layer_weight\n",
    "\n",
    "                        relevancy_scores.append(weighted_attn)\n",
    "\n",
    "        if relevancy_scores:\n",
    "            # Aggregate across layers\n",
    "            final_relevancy = torch.stack(relevancy_scores).mean(dim=0)\n",
    "            return final_relevancy.reshape(16, 16).numpy()\n",
    "        else:\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "    def compute_head_importance_relevancy(self, outputs, inputs, token_idx):\n",
    "        \"\"\"\n",
    "        Compute relevancy by identifying important heads first\n",
    "        \"\"\"\n",
    "\n",
    "        image_start, image_end = 1, 257\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "        # Get last layer attention for head importance\n",
    "        last_layer_attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "\n",
    "        if len(last_layer_attn.shape) == 4:\n",
    "            last_layer_attn = last_layer_attn[0]  # Remove batch\n",
    "\n",
    "        num_heads = last_layer_attn.shape[0]\n",
    "        head_importance = []\n",
    "\n",
    "        # Calculate importance score for each head\n",
    "        for h in range(num_heads):\n",
    "            head_attn = last_layer_attn[h]\n",
    "\n",
    "            # Find appropriate position\n",
    "            src_pos = min(input_length + token_idx, head_attn.shape[0] - 1)\n",
    "\n",
    "            if src_pos >= 0 and image_end <= head_attn.shape[1]:\n",
    "                attn_to_image = head_attn[src_pos, image_start:image_end]\n",
    "\n",
    "                # Importance = max attention * entropy (focused but strong)\n",
    "                max_attn = attn_to_image.max().item()\n",
    "                entropy = -(attn_to_image * torch.log(attn_to_image + 1e-10)).sum().item()\n",
    "                importance = max_attn * (1 / (1 + entropy))\n",
    "\n",
    "                head_importance.append((h, importance, attn_to_image))\n",
    "\n",
    "        # Sort by importance\n",
    "        head_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Use top heads\n",
    "        top_k = min(4, len(head_importance))\n",
    "        relevancy_map = torch.zeros(256)\n",
    "\n",
    "        for h, importance, attn in head_importance[:top_k]:\n",
    "            relevancy_map += attn * importance\n",
    "\n",
    "        # Normalize\n",
    "        if relevancy_map.max() > 0:\n",
    "            relevancy_map = relevancy_map / relevancy_map.max()\n",
    "\n",
    "        return relevancy_map.reshape(16, 16).numpy()\n",
    "\n",
    "    def compute_attention_flow(self, outputs, inputs, token_idx):\n",
    "        \"\"\"\n",
    "        Trace attention flow from output to input through layers\n",
    "        More robust than matrix multiplication\n",
    "        \"\"\"\n",
    "\n",
    "        image_start, image_end = 1, 257\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "        # Start from the last layer\n",
    "        token_attentions = outputs.attentions[token_idx]\n",
    "\n",
    "        # Initialize flow with last layer attention\n",
    "        last_layer = token_attentions[-1].cpu().float()\n",
    "        if len(last_layer.shape) == 4:\n",
    "            last_layer = last_layer[0]\n",
    "\n",
    "        # Average over heads\n",
    "        flow = last_layer.mean(dim=0)\n",
    "\n",
    "        # Find source position\n",
    "        src_pos = min(input_length + token_idx, flow.shape[0] - 1)\n",
    "\n",
    "        # Extract attention to image\n",
    "        if src_pos >= 0 and image_end <= flow.shape[1]:\n",
    "            image_attention = flow[src_pos, image_start:image_end]\n",
    "        else:\n",
    "            image_attention = torch.zeros(256)\n",
    "\n",
    "        # Weight by layer contributions (backward through layers)\n",
    "        layer_contributions = []\n",
    "\n",
    "        for layer_idx in range(len(token_attentions) - 2, -1, -1):\n",
    "            layer_attn = token_attentions[layer_idx].cpu().float()\n",
    "            if len(layer_attn.shape) == 4:\n",
    "                layer_attn = layer_attn[0]\n",
    "\n",
    "            # Average over heads\n",
    "            layer_attn = layer_attn.mean(dim=0)\n",
    "\n",
    "            # Sample attention values to image region\n",
    "            if src_pos < layer_attn.shape[0] and image_end <= layer_attn.shape[1]:\n",
    "                layer_contribution = layer_attn[src_pos, image_start:image_end]\n",
    "                layer_contributions.append(layer_contribution)\n",
    "\n",
    "        # Combine contributions\n",
    "        if layer_contributions:\n",
    "            # Average with decreasing weights for earlier layers\n",
    "            weights = torch.tensor([0.5 ** i for i in range(len(layer_contributions))])\n",
    "            weights = weights / weights.sum()\n",
    "\n",
    "            combined = image_attention * 0.5  # Last layer gets 50%\n",
    "            for i, contrib in enumerate(layer_contributions):\n",
    "                combined = combined + contrib * weights[i] * 0.5\n",
    "\n",
    "            return combined.reshape(16, 16).numpy()\n",
    "\n",
    "        return image_attention.reshape(16, 16).numpy()\n",
    "\n",
    "\n",
    "def extract_raw_attention_safe(outputs, inputs, token_idx):\n",
    "    \"\"\"Safely extract raw attention with error handling\"\"\"\n",
    "    try:\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            token_idx = len(outputs.attentions) - 1\n",
    "\n",
    "        attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "        if len(attn.shape) == 4:\n",
    "            attn = attn[0]\n",
    "\n",
    "        avg_attn = attn.mean(dim=0)\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        # Handle fixed attention matrix size\n",
    "        if avg_attn.shape[0] == avg_attn.shape[1]:\n",
    "            # Square matrix - find appropriate position\n",
    "            gen_pos = min(input_len + token_idx, avg_attn.shape[0] - 1)\n",
    "        else:\n",
    "            gen_pos = -1\n",
    "\n",
    "        if gen_pos >= 0 and 257 <= avg_attn.shape[1]:\n",
    "            attn_to_image = avg_attn[gen_pos, 1:257]\n",
    "            return attn_to_image.reshape(16, 16).numpy()\n",
    "        else:\n",
    "            print(f\"Warning: Could not extract attention for token {token_idx}\")\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting raw attention: {e}\")\n",
    "        return np.zeros((16, 16))\n",
    "\n",
    "\n",
    "def visualize_relevancy_methods(chest_xray, raw_attn, simple_rel, head_rel, flow_rel, title=\"\"):\n",
    "    \"\"\"Visualize all working methods\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Original image\n",
    "    axes[0, 0].imshow(chest_xray, cmap='gray')\n",
    "    axes[0, 0].set_title('Input X-ray')\n",
    "    axes[0, 0].axis('off')\n",
    "\n",
    "    # Raw attention\n",
    "    im1 = axes[0, 1].imshow(raw_attn, cmap='hot', interpolation='bicubic')\n",
    "    axes[0, 1].set_title('Raw Attention')\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "    # Simple relevancy\n",
    "    im2 = axes[0, 2].imshow(simple_rel, cmap='jet', interpolation='bicubic')\n",
    "    axes[0, 2].set_title('Layer-Weighted Relevancy')\n",
    "    axes[0, 2].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "    # Head importance\n",
    "    im3 = axes[1, 0].imshow(head_rel, cmap='plasma', interpolation='bicubic')\n",
    "    axes[1, 0].set_title('Head Importance Relevancy')\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[1, 0], fraction=0.046)\n",
    "\n",
    "    # Attention flow\n",
    "    im4 = axes[1, 1].imshow(flow_rel, cmap='viridis', interpolation='bicubic')\n",
    "    axes[1, 1].set_title('Attention Flow')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im4, ax=axes[1, 1], fraction=0.046)\n",
    "\n",
    "    # Consensus (average of all methods)\n",
    "    consensus = (simple_rel + head_rel + flow_rel) / 3\n",
    "    im5 = axes[1, 2].imshow(consensus, cmap='RdYlBu_r', interpolation='bicubic')\n",
    "    axes[1, 2].set_title('Consensus Relevancy')\n",
    "    axes[1, 2].axis('off')\n",
    "    plt.colorbar(im5, ax=axes[1, 2], fraction=0.046)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def analyze_medgemma_relevancy(model, processor, inputs, outputs, chest_xray,\n",
    "                               medical_report, token_idx=0):\n",
    "    \"\"\"Main analysis function with all working methods\"\"\"\n",
    "\n",
    "    print(f\"\\n=== ANALYZING TOKEN {token_idx} ===\")\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = MedGemmaRelevancyAnalyzer(model, processor)\n",
    "\n",
    "    # 1. Extract raw attention\n",
    "    print(\"1. Extracting raw attention...\")\n",
    "    raw_attention = extract_raw_attention_safe(outputs, inputs, token_idx)\n",
    "\n",
    "    # 2. Compute simple relevancy\n",
    "    print(\"2. Computing layer-weighted relevancy...\")\n",
    "    simple_relevancy = analyzer.compute_simple_relevancy(outputs, inputs, token_idx)\n",
    "\n",
    "    # 3. Compute head importance relevancy\n",
    "    print(\"3. Computing head importance relevancy...\")\n",
    "    head_relevancy = analyzer.compute_head_importance_relevancy(outputs, inputs, token_idx)\n",
    "\n",
    "    # 4. Compute attention flow\n",
    "    print(\"4. Computing attention flow...\")\n",
    "    flow_relevancy = analyzer.compute_attention_flow(outputs, inputs, token_idx)\n",
    "\n",
    "    # 5. Visualize all methods\n",
    "    print(\"5. Creating visualizations...\")\n",
    "    fig = visualize_relevancy_methods(\n",
    "        chest_xray, raw_attention, simple_relevancy,\n",
    "        head_relevancy, flow_relevancy,\n",
    "        title=f\"Relevancy Analysis for Token {token_idx}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Analysis summary\n",
    "    print(\"\\n📊 Analysis Summary:\")\n",
    "    print(f\"Raw attention     - Min: {raw_attention.min():.4f}, Max: {raw_attention.max():.4f}\")\n",
    "    print(f\"Simple relevancy  - Min: {simple_relevancy.min():.4f}, Max: {simple_relevancy.max():.4f}\")\n",
    "    print(f\"Head relevancy    - Min: {head_relevancy.min():.4f}, Max: {head_relevancy.max():.4f}\")\n",
    "    print(f\"Flow relevancy    - Min: {flow_relevancy.min():.4f}, Max: {flow_relevancy.max():.4f}\")\n",
    "\n",
    "    # Find regions of agreement\n",
    "    consensus = (simple_relevancy + head_relevancy + flow_relevancy) / 3\n",
    "    threshold = np.percentile(consensus, 80)\n",
    "    high_relevance_mask = consensus > threshold\n",
    "\n",
    "    print(f\"\\n🎯 High relevance regions (>80th percentile):\")\n",
    "    y_coords, x_coords = np.where(high_relevance_mask)\n",
    "    if len(y_coords) > 0:\n",
    "        center_y = y_coords.mean() / 16\n",
    "        center_x = x_coords.mean() / 16\n",
    "\n",
    "        if center_y < 0.33:\n",
    "            v_pos = \"Upper\"\n",
    "        elif center_y > 0.67:\n",
    "            v_pos = \"Lower\"\n",
    "        else:\n",
    "            v_pos = \"Middle\"\n",
    "\n",
    "        if center_x < 0.33:\n",
    "            h_pos = \"left\"\n",
    "        elif center_x > 0.67:\n",
    "            h_pos = \"right\"\n",
    "        else:\n",
    "            h_pos = \"center\"\n",
    "\n",
    "        print(f\"Primary focus: {v_pos} {h_pos} region\")\n",
    "\n",
    "    return {\n",
    "        'raw_attention': raw_attention,\n",
    "        'simple_relevancy': simple_relevancy,\n",
    "        'head_relevancy': head_relevancy,\n",
    "        'flow_relevancy': flow_relevancy,\n",
    "        'consensus': consensus\n",
    "    }\n",
    "\n",
    "\n",
    "def run_complete_analysis(model, processor, inputs_gpu, outputs, chest_xray, medical_report):\n",
    "    \"\"\"Run complete analysis on multiple tokens\"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"MEDGEMMA RELEVANCY ANALYSIS (FIXED)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Analyze first few tokens\n",
    "    all_results = {}\n",
    "    tokens_to_analyze = [0, 2, 5]\n",
    "\n",
    "    for token_idx in tokens_to_analyze:\n",
    "        if token_idx < len(outputs.attentions):\n",
    "            results = analyze_medgemma_relevancy(\n",
    "                model, processor, inputs_gpu, outputs,\n",
    "                chest_xray, medical_report, token_idx\n",
    "            )\n",
    "            all_results[token_idx] = results\n",
    "\n",
    "    # Create evolution visualization\n",
    "    if len(all_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ATTENTION EVOLUTION ACROSS TOKENS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        fig, axes = plt.subplots(len(all_results), 3, figsize=(12, 4*len(all_results)))\n",
    "\n",
    "        for i, (token_idx, results) in enumerate(all_results.items()):\n",
    "            # Raw attention\n",
    "            axes[i, 0].imshow(results['raw_attention'], cmap='hot', interpolation='bicubic')\n",
    "            axes[i, 0].set_title(f'Token {token_idx}: Raw Attention')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # Consensus relevancy\n",
    "            axes[i, 1].imshow(results['consensus'], cmap='jet', interpolation='bicubic')\n",
    "            axes[i, 1].set_title(f'Token {token_idx}: Consensus Relevancy')\n",
    "            axes[i, 1].axis('off')\n",
    "\n",
    "            # Difference\n",
    "            diff = results['consensus'] - results['raw_attention']\n",
    "            axes[i, 2].imshow(diff, cmap='RdBu_r', interpolation='bicubic')\n",
    "            axes[i, 2].set_title(f'Token {token_idx}: Relevancy - Raw')\n",
    "            axes[i, 2].axis('off')\n",
    "\n",
    "        plt.suptitle('Evolution of Attention Across Generated Tokens', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n✅ Analysis complete!\")\n",
    "    print(\"\\nKey findings:\")\n",
    "    print(\"- Layer-weighted relevancy shows contribution across all layers\")\n",
    "    print(\"- Head importance identifies most informative attention heads\")\n",
    "    print(\"- Attention flow traces information from output back to input\")\n",
    "    print(\"- Consensus relevancy combines all methods for robust results\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting fixed MedGemma relevancy analysis...\")\n",
    "\n",
    "    # Run the analysis\n",
    "    results = run_complete_analysis(\n",
    "        model, processor, inputs_gpu, outputs,\n",
    "        xray_pil_image, medical_report\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ All analyses completed successfully!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WrITMv7voNpU",
    "outputId": "fb506b5d-5ad6-400a-e476-e513b685a4c8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MedGemmaRelevancyAnalyzer:\n",
    "    \"\"\"\n",
    "    Complete implementation for MedGemma's relevancy analysis\n",
    "    Handles bfloat16, fixed attention matrices, and provides multiple relevancy methods\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.model.eval()\n",
    "\n",
    "    def compute_simple_relevancy(self, outputs, inputs, token_idx):\n",
    "        \"\"\"\n",
    "        Simplified relevancy using layer-weighted attention\n",
    "        Works with MedGemma's architecture\n",
    "        \"\"\"\n",
    "\n",
    "        image_start, image_end = 1, 257\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        relevancy_scores = []\n",
    "\n",
    "        # Process attention from each layer\n",
    "        if token_idx < len(outputs.attentions):\n",
    "            token_attentions = outputs.attentions[token_idx]\n",
    "\n",
    "            for layer_idx, layer_attn in enumerate(token_attentions):\n",
    "                if torch.is_tensor(layer_attn):\n",
    "                    # Convert to float32 for computation\n",
    "                    layer_attn = layer_attn.cpu().float()\n",
    "\n",
    "                    if len(layer_attn.shape) == 4:\n",
    "                        layer_attn = layer_attn[0]  # Remove batch\n",
    "\n",
    "                    # Average over heads\n",
    "                    layer_attn = layer_attn.mean(dim=0)\n",
    "\n",
    "                    # MedGemma uses fixed attention size\n",
    "                    # Find the right position to extract from\n",
    "                    if layer_attn.shape[0] == layer_attn.shape[1]:\n",
    "                        # Square attention matrix\n",
    "                        if layer_attn.shape[0] > input_length:\n",
    "                            # Use last position that makes sense\n",
    "                            src_pos = min(input_length + token_idx, layer_attn.shape[0] - 1)\n",
    "                        else:\n",
    "                            src_pos = layer_attn.shape[0] - 1\n",
    "                    else:\n",
    "                        src_pos = -1\n",
    "\n",
    "                    # Extract attention to image tokens\n",
    "                    if src_pos >= 0 and image_end <= layer_attn.shape[1]:\n",
    "                        attn_to_image = layer_attn[src_pos, image_start:image_end]\n",
    "\n",
    "                        # Weight by layer depth (later layers more important)\n",
    "                        layer_weight = (layer_idx + 1) / len(token_attentions)\n",
    "                        weighted_attn = attn_to_image * layer_weight\n",
    "\n",
    "                        relevancy_scores.append(weighted_attn)\n",
    "\n",
    "        if relevancy_scores:\n",
    "            # Aggregate across layers\n",
    "            final_relevancy = torch.stack(relevancy_scores).mean(dim=0)\n",
    "            return final_relevancy.reshape(16, 16).numpy()\n",
    "        else:\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "    def compute_head_importance_relevancy(self, outputs, inputs, token_idx):\n",
    "        \"\"\"\n",
    "        Compute relevancy by identifying important heads first\n",
    "        Fixed version with better normalization\n",
    "        \"\"\"\n",
    "\n",
    "        image_start, image_end = 1, 257\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "        # Get last layer attention for head importance\n",
    "        last_layer_attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "\n",
    "        if len(last_layer_attn.shape) == 4:\n",
    "            last_layer_attn = last_layer_attn[0]  # Remove batch\n",
    "\n",
    "        num_heads = last_layer_attn.shape[0]\n",
    "        head_importance = []\n",
    "\n",
    "        # Calculate importance score for each head\n",
    "        for h in range(num_heads):\n",
    "            head_attn = last_layer_attn[h]\n",
    "\n",
    "            # Find appropriate position\n",
    "            src_pos = min(input_length + token_idx, head_attn.shape[0] - 1)\n",
    "\n",
    "            if src_pos >= 0 and image_end <= head_attn.shape[1]:\n",
    "                attn_to_image = head_attn[src_pos, image_start:image_end]\n",
    "\n",
    "                # Importance = max attention * entropy (focused but strong)\n",
    "                max_attn = attn_to_image.max().item()\n",
    "                entropy = -(attn_to_image * torch.log(attn_to_image + 1e-10)).sum().item()\n",
    "                importance = max_attn * (1 / (1 + entropy))\n",
    "\n",
    "                head_importance.append((h, importance, attn_to_image))\n",
    "\n",
    "        # Sort by importance\n",
    "        head_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Use top heads\n",
    "        top_k = min(4, len(head_importance))\n",
    "        relevancy_map = torch.zeros(256)\n",
    "\n",
    "        for h, importance, attn in head_importance[:top_k]:\n",
    "            relevancy_map += attn * importance\n",
    "\n",
    "        # Better normalization - use percentile instead of max\n",
    "        if relevancy_map.max() > 0:\n",
    "            p95 = torch.quantile(relevancy_map, 0.95)\n",
    "            if p95 > 0:\n",
    "                relevancy_map = torch.clamp(relevancy_map / p95, 0, 1)\n",
    "            else:\n",
    "                relevancy_map = relevancy_map / relevancy_map.max()\n",
    "\n",
    "        return relevancy_map.reshape(16, 16).numpy()\n",
    "\n",
    "    def compute_attention_flow(self, outputs, inputs, token_idx):\n",
    "        \"\"\"\n",
    "        Trace attention flow from output to input through layers\n",
    "        More robust than matrix multiplication\n",
    "        \"\"\"\n",
    "\n",
    "        image_start, image_end = 1, 257\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "        # Start from the last layer\n",
    "        token_attentions = outputs.attentions[token_idx]\n",
    "\n",
    "        # Initialize flow with last layer attention\n",
    "        last_layer = token_attentions[-1].cpu().float()\n",
    "        if len(last_layer.shape) == 4:\n",
    "            last_layer = last_layer[0]\n",
    "\n",
    "        # Average over heads\n",
    "        flow = last_layer.mean(dim=0)\n",
    "\n",
    "        # Find source position\n",
    "        src_pos = min(input_length + token_idx, flow.shape[0] - 1)\n",
    "\n",
    "        # Extract attention to image\n",
    "        if src_pos >= 0 and image_end <= flow.shape[1]:\n",
    "            image_attention = flow[src_pos, image_start:image_end]\n",
    "        else:\n",
    "            image_attention = torch.zeros(256)\n",
    "\n",
    "        # Weight by layer contributions (backward through layers)\n",
    "        layer_contributions = []\n",
    "\n",
    "        for layer_idx in range(len(token_attentions) - 2, -1, -1):\n",
    "            layer_attn = token_attentions[layer_idx].cpu().float()\n",
    "            if len(layer_attn.shape) == 4:\n",
    "                layer_attn = layer_attn[0]\n",
    "\n",
    "            # Average over heads\n",
    "            layer_attn = layer_attn.mean(dim=0)\n",
    "\n",
    "            # Sample attention values to image region\n",
    "            if src_pos < layer_attn.shape[0] and image_end <= layer_attn.shape[1]:\n",
    "                layer_contribution = layer_attn[src_pos, image_start:image_end]\n",
    "                layer_contributions.append(layer_contribution)\n",
    "\n",
    "        # Combine contributions\n",
    "        if layer_contributions:\n",
    "            # Average with decreasing weights for earlier layers\n",
    "            weights = torch.tensor([0.5 ** i for i in range(len(layer_contributions))])\n",
    "            weights = weights / weights.sum()\n",
    "\n",
    "            combined = image_attention * 0.5  # Last layer gets 50%\n",
    "            for i, contrib in enumerate(layer_contributions):\n",
    "                combined = combined + contrib * weights[i] * 0.5\n",
    "\n",
    "            return combined.reshape(16, 16).numpy()\n",
    "\n",
    "        return image_attention.reshape(16, 16).numpy()\n",
    "\n",
    "\n",
    "def extract_raw_attention_safe(outputs, inputs, token_idx):\n",
    "    \"\"\"Safely extract raw attention with error handling\"\"\"\n",
    "    try:\n",
    "        if token_idx >= len(outputs.attentions):\n",
    "            token_idx = len(outputs.attentions) - 1\n",
    "\n",
    "        attn = outputs.attentions[token_idx][-1].cpu().float()\n",
    "        if len(attn.shape) == 4:\n",
    "            attn = attn[0]\n",
    "\n",
    "        avg_attn = attn.mean(dim=0)\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "        # Handle fixed attention matrix size\n",
    "        if avg_attn.shape[0] == avg_attn.shape[1]:\n",
    "            # Square matrix - find appropriate position\n",
    "            gen_pos = min(input_len + token_idx, avg_attn.shape[0] - 1)\n",
    "        else:\n",
    "            gen_pos = -1\n",
    "\n",
    "        if gen_pos >= 0 and 257 <= avg_attn.shape[1]:\n",
    "            attn_to_image = avg_attn[gen_pos, 1:257]\n",
    "            return attn_to_image.reshape(16, 16).numpy()\n",
    "        else:\n",
    "            print(f\"Warning: Could not extract attention for token {token_idx}\")\n",
    "            return np.zeros((16, 16))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting raw attention: {e}\")\n",
    "        return np.zeros((16, 16))\n",
    "\n",
    "\n",
    "def enhance_visualization_contrast(attention_map, method='percentile'):\n",
    "    \"\"\"Enhance contrast for better visibility\"\"\"\n",
    "\n",
    "    if method == 'percentile':\n",
    "        # Use 5th-95th percentile for better contrast\n",
    "        p5, p95 = np.percentile(attention_map, [5, 95])\n",
    "        if p95 > p5:\n",
    "            enhanced = np.clip((attention_map - p5) / (p95 - p5), 0, 1)\n",
    "        else:\n",
    "            enhanced = attention_map\n",
    "    elif method == 'log':\n",
    "        # Log scale for very small values\n",
    "        enhanced = np.log(attention_map + 1e-8)\n",
    "        enhanced = (enhanced - enhanced.min()) / (enhanced.max() - enhanced.min() + 1e-8)\n",
    "    else:\n",
    "        enhanced = attention_map\n",
    "\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def visualize_relevancy_methods(chest_xray, raw_attn, simple_rel, head_rel, flow_rel, title=\"\"):\n",
    "    \"\"\"Visualize all working methods with enhanced contrast\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Original image\n",
    "    axes[0, 0].imshow(chest_xray, cmap='gray')\n",
    "    axes[0, 0].set_title('Input X-ray')\n",
    "    axes[0, 0].axis('off')\n",
    "\n",
    "    # Raw attention (enhanced)\n",
    "    raw_enhanced = enhance_visualization_contrast(raw_attn)\n",
    "    im1 = axes[0, 1].imshow(raw_enhanced, cmap='hot', interpolation='bicubic')\n",
    "    axes[0, 1].set_title('Raw Attention')\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "    # Simple relevancy (enhanced)\n",
    "    simple_enhanced = enhance_visualization_contrast(simple_rel)\n",
    "    im2 = axes[0, 2].imshow(simple_enhanced, cmap='jet', interpolation='bicubic')\n",
    "    axes[0, 2].set_title('Layer-Weighted Relevancy')\n",
    "    axes[0, 2].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "    # Head importance (already normalized)\n",
    "    im3 = axes[1, 0].imshow(head_rel, cmap='plasma', interpolation='bicubic')\n",
    "    axes[1, 0].set_title('Head Importance Relevancy')\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im3, ax=axes[1, 0], fraction=0.046)\n",
    "\n",
    "    # Attention flow (enhanced)\n",
    "    flow_enhanced = enhance_visualization_contrast(flow_rel)\n",
    "    im4 = axes[1, 1].imshow(flow_enhanced, cmap='viridis', interpolation='bicubic')\n",
    "    axes[1, 1].set_title('Attention Flow')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im4, ax=axes[1, 1], fraction=0.046)\n",
    "\n",
    "    # Consensus (average of all methods)\n",
    "    consensus = (simple_enhanced + head_rel + flow_enhanced) / 3\n",
    "    im5 = axes[1, 2].imshow(consensus, cmap='RdYlBu_r', interpolation='bicubic')\n",
    "    axes[1, 2].set_title('Consensus Relevancy')\n",
    "    axes[1, 2].axis('off')\n",
    "    plt.colorbar(im5, ax=axes[1, 2], fraction=0.046)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig, consensus\n",
    "\n",
    "\n",
    "def create_attention_overlay(chest_xray, attention_map, title=\"Attention Overlay\"):\n",
    "    \"\"\"Create clean overlay visualization\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Resize attention to image size\n",
    "    h, w = chest_xray.size[::-1]\n",
    "    attention_resized = cv2.resize(attention_map, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # 1. Original\n",
    "    axes[0].imshow(chest_xray, cmap='gray')\n",
    "    axes[0].set_title('Original X-ray')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 2. Attention overlay\n",
    "    axes[1].imshow(chest_xray, cmap='gray')\n",
    "    axes[1].imshow(attention_resized, cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(title)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 3. Contour regions\n",
    "    threshold = np.percentile(attention_resized, 85)\n",
    "    mask = attention_resized > threshold\n",
    "\n",
    "    axes[2].imshow(chest_xray, cmap='gray')\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 100:  # Filter small regions\n",
    "            axes[2].add_patch(plt.Circle((0, 0), 0))  # Dummy for color\n",
    "            contour = contour.squeeze()\n",
    "            if len(contour.shape) == 2 and contour.shape[0] > 2:\n",
    "                axes[2].plot(contour[:, 0], contour[:, 1], 'r-', linewidth=2)\n",
    "\n",
    "    axes[2].set_title('High Attention Regions')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def analyze_medgemma_relevancy(model, processor, inputs, outputs, chest_xray,\n",
    "                               medical_report, token_idx=0):\n",
    "    \"\"\"Main analysis function with all working methods\"\"\"\n",
    "\n",
    "    print(f\"\\n=== ANALYZING TOKEN {token_idx} ===\")\n",
    "\n",
    "    # Initialize analyzer\n",
    "    analyzer = MedGemmaRelevancyAnalyzer(model, processor)\n",
    "\n",
    "    # 1. Extract raw attention\n",
    "    print(\"1. Extracting raw attention...\")\n",
    "    raw_attention = extract_raw_attention_safe(outputs, inputs, token_idx)\n",
    "\n",
    "    # 2. Compute simple relevancy\n",
    "    print(\"2. Computing layer-weighted relevancy...\")\n",
    "    simple_relevancy = analyzer.compute_simple_relevancy(outputs, inputs, token_idx)\n",
    "\n",
    "    # 3. Compute head importance relevancy\n",
    "    print(\"3. Computing head importance relevancy...\")\n",
    "    head_relevancy = analyzer.compute_head_importance_relevancy(outputs, inputs, token_idx)\n",
    "\n",
    "    # 4. Compute attention flow\n",
    "    print(\"4. Computing attention flow...\")\n",
    "    flow_relevancy = analyzer.compute_attention_flow(outputs, inputs, token_idx)\n",
    "\n",
    "    # 5. Visualize all methods\n",
    "    print(\"5. Creating visualizations...\")\n",
    "    fig, consensus = visualize_relevancy_methods(\n",
    "        chest_xray, raw_attention, simple_relevancy,\n",
    "        head_relevancy, flow_relevancy,\n",
    "        title=f\"Relevancy Analysis for Token {token_idx}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Create overlay for consensus\n",
    "    overlay_fig = create_attention_overlay(chest_xray, consensus,\n",
    "                                         f\"Consensus Attention - Token {token_idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    # 7. Analysis summary\n",
    "    print(\"\\n📊 Analysis Summary:\")\n",
    "    print(f\"Raw attention     - Min: {raw_attention.min():.4f}, Max: {raw_attention.max():.4f}\")\n",
    "    print(f\"Simple relevancy  - Min: {simple_relevancy.min():.4f}, Max: {simple_relevancy.max():.4f}\")\n",
    "    print(f\"Head relevancy    - Min: {head_relevancy.min():.4f}, Max: {head_relevancy.max():.4f}\")\n",
    "    print(f\"Flow relevancy    - Min: {flow_relevancy.min():.4f}, Max: {flow_relevancy.max():.4f}\")\n",
    "\n",
    "    # Find regions of agreement\n",
    "    threshold = np.percentile(consensus, 80)\n",
    "    high_relevance_mask = consensus > threshold\n",
    "\n",
    "    print(f\"\\n🎯 High relevance regions (>80th percentile):\")\n",
    "    y_coords, x_coords = np.where(high_relevance_mask)\n",
    "    if len(y_coords) > 0:\n",
    "        center_y = y_coords.mean() / 16\n",
    "        center_x = x_coords.mean() / 16\n",
    "\n",
    "        if center_y < 0.33:\n",
    "            v_pos = \"Upper\"\n",
    "        elif center_y > 0.67:\n",
    "            v_pos = \"Lower\"\n",
    "        else:\n",
    "            v_pos = \"Middle\"\n",
    "\n",
    "        if center_x < 0.33:\n",
    "            h_pos = \"left\"\n",
    "        elif center_x > 0.67:\n",
    "            h_pos = \"right\"\n",
    "        else:\n",
    "            h_pos = \"center\"\n",
    "\n",
    "        print(f\"Primary focus: {v_pos} {h_pos} region\")\n",
    "\n",
    "    return {\n",
    "        'raw_attention': raw_attention,\n",
    "        'simple_relevancy': simple_relevancy,\n",
    "        'head_relevancy': head_relevancy,\n",
    "        'flow_relevancy': flow_relevancy,\n",
    "        'consensus': consensus\n",
    "    }\n",
    "\n",
    "\n",
    "def run_complete_analysis(model, processor, inputs_gpu, outputs, chest_xray, medical_report):\n",
    "    \"\"\"Run complete analysis on multiple tokens\"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"MEDGEMMA RELEVANCY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Analyze first few tokens\n",
    "    all_results = {}\n",
    "    tokens_to_analyze = [0, 2, 5]\n",
    "\n",
    "    for token_idx in tokens_to_analyze:\n",
    "        if token_idx < len(outputs.attentions):\n",
    "            results = analyze_medgemma_relevancy(\n",
    "                model, processor, inputs_gpu, outputs,\n",
    "                chest_xray, medical_report, token_idx\n",
    "            )\n",
    "            all_results[token_idx] = results\n",
    "\n",
    "    # Create evolution visualization\n",
    "    if len(all_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ATTENTION EVOLUTION ACROSS TOKENS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        fig, axes = plt.subplots(len(all_results), 3, figsize=(12, 4*len(all_results)))\n",
    "\n",
    "        for i, (token_idx, results) in enumerate(all_results.items()):\n",
    "            # Raw attention (enhanced)\n",
    "            raw_enhanced = enhance_visualization_contrast(results['raw_attention'])\n",
    "            axes[i, 0].imshow(raw_enhanced, cmap='hot', interpolation='bicubic')\n",
    "            axes[i, 0].set_title(f'Token {token_idx}: Raw Attention')\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            # Consensus relevancy\n",
    "            axes[i, 1].imshow(results['consensus'], cmap='jet', interpolation='bicubic')\n",
    "            axes[i, 1].set_title(f'Token {token_idx}: Consensus Relevancy')\n",
    "            axes[i, 1].axis('off')\n",
    "\n",
    "            # Difference\n",
    "            diff = results['consensus'] - raw_enhanced\n",
    "            axes[i, 2].imshow(diff, cmap='RdBu_r', interpolation='bicubic',\n",
    "                            vmin=-0.5, vmax=0.5)\n",
    "            axes[i, 2].set_title(f'Token {token_idx}: Relevancy - Raw')\n",
    "            axes[i, 2].axis('off')\n",
    "\n",
    "        plt.suptitle('Evolution of Attention Across Generated Tokens', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for token_idx, results in all_results.items():\n",
    "        consensus = results['consensus']\n",
    "        print(f\"\\nToken {token_idx}:\")\n",
    "        print(f\"  Consensus mean: {consensus.mean():.4f}\")\n",
    "        print(f\"  Consensus std:  {consensus.std():.4f}\")\n",
    "        print(f\"  Max location:   {np.unravel_index(consensus.argmax(), consensus.shape)}\")\n",
    "\n",
    "    print(\"\\n✅ Analysis complete!\")\n",
    "    print(\"\\nKey findings:\")\n",
    "    print(\"- Layer-weighted relevancy integrates contributions across all 34 layers\")\n",
    "    print(\"- Head importance identifies the most informative of 8 attention heads\")\n",
    "    print(\"- Attention flow traces information propagation without matrix multiplication\")\n",
    "    print(\"- Consensus relevancy provides robust results by combining all methods\")\n",
    "    print(\"\\nRelevancy maps show more focused and interpretable patterns than raw attention!\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Integration with existing code\n",
    "def analyze_chest_xray_with_relevancy(model, processor, inputs_gpu, outputs,\n",
    "                                     chest_xray, medical_report):\n",
    "    \"\"\"\n",
    "    Easy integration function for existing pipelines\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting MedGemma relevancy analysis...\")\n",
    "\n",
    "    # Run the complete analysis\n",
    "    results = run_complete_analysis(\n",
    "        model, processor, inputs_gpu, outputs,\n",
    "        chest_xray, medical_report\n",
    "    )\n",
    "\n",
    "    # Save results if needed\n",
    "    try:\n",
    "        import pickle\n",
    "        with open('medgemma_relevancy_results.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(\"\\n✓ Results saved to medgemma_relevancy_results.pkl\")\n",
    "    except:\n",
    "        print(\"\\n⚠️ Could not save results\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # This integrates with your existing code\n",
    "    # Assuming you have already:\n",
    "    # - Loaded the model\n",
    "    # - Generated outputs with attention\n",
    "    # - Have chest_xray (PIL Image) and medical_report (string)\n",
    "\n",
    "    results = analyze_chest_xray_with_relevancy(\n",
    "        model, processor, inputs_gpu, outputs,\n",
    "        xray_pil_image, medical_report  # Use your variable names\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ All analyses completed successfully!\")\n",
    "    print(\"Relevancy maps provide deeper insights than raw attention alone!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mmBOv6f-siHw",
    "outputId": "2f6bd520-05f1-40cf-a849-6d580f5f8509"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
